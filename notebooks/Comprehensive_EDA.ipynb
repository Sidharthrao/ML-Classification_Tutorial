{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection - Comprehensive Exploratory Data Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "This notebook demonstrates industry-standard EDA techniques for fraud detection:\n",
    "- **Data Quality Assessment**: Identify missing values, duplicates, and data type issues\n",
    "- **Univariate Analysis**: Understand individual feature distributions\n",
    "- **Bivariate Analysis**: Explore relationships between features and target\n",
    "- **Multivariate Analysis**: Discover complex patterns and interactions\n",
    "- **Feature Engineering Opportunities**: Identify potential new features\n",
    "\n",
    "## Context\n",
    "Fraud detection is a critical application of machine learning in finance. The extreme class imbalance (0.13% fraud rate) presents unique challenges that require specialized analytical approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Core libraries for data manipulation and analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlite3\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Core libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "DATABASE_PATH = '/Users/sidharthrao/Documents/Documents_Sid MacBook Pro/GitHub/Project-Rogue/Inttrvu/Capstone_Projects/Database.db'\n",
    "SAMPLE_SIZE = 100000  # For memory efficiency in visualizations\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fraud_data(sample_size=None):\n",
    "    \"\"\"\n",
    "    Load fraud detection data from SQLite database\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int, optional\n",
    "        Number of rows to sample for memory efficiency\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded fraud detection data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DATABASE_PATH)\n",
    "        \n",
    "        if sample_size:\n",
    "            query = f\"SELECT * FROM Fraud_detection ORDER BY RANDOM() LIMIT {sample_size}\"\n",
    "        else:\n",
    "            query = \"SELECT * FROM Fraud_detection\"\n",
    "            \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully! Shape: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data with sampling for efficient processing\n",
    "df = load_fraud_data(SAMPLE_SIZE)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"- Total Records: {df.shape[0]:,}\")\n",
    "    print(f\"- Features: {df.shape[1]}\")\n",
    "    print(f\"- Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types(df):\n",
    "    \"\"\"\n",
    "    Convert columns to appropriate data types for analysis\n",
    "    \n",
    "    Learning Note: Data type conversion is crucial for:\n",
    "    - Memory efficiency\n",
    "    - Correct statistical analysis\n",
    "    - Proper visualization\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                   'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df_converted[col] = pd.to_numeric(df_converted[col], errors='coerce')\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    categorical_cols = ['type', 'nameOrig', 'nameDest']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        df_converted[col] = df_converted[col].astype('category')\n",
    "    \n",
    "    print(\"‚úÖ Data types converted successfully!\")\n",
    "    return df_converted\n",
    "\n",
    "if df is not None:\n",
    "    df = convert_data_types(df)\n",
    "    print(\"\\nüìã Data Types After Conversion:\")\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment\n",
    "    \n",
    "    Learning Note: Data quality is foundational for reliable ML models.\n",
    "    Poor data quality leads to:\n",
    "    - Biased model predictions\n",
    "    - Poor generalization\n",
    "    - Incorrect business insights\n",
    "    \"\"\"\n",
    "    print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Missing values analysis\n",
    "    print(\"\\n1. Missing Values Analysis:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_values,\n",
    "        'Missing Percentage': missing_percentage\n",
    "    })\n",
    "    \n",
    "    if missing_df['Missing Count'].sum() > 0:\n",
    "        print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "    \n",
    "    # Duplicate records analysis\n",
    "    print(\"\\n2. Duplicate Records Analysis:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"- Duplicate Records: {duplicates:,} ({(duplicates/len(df))*100:.4f}%)\")\n",
    "    \n",
    "    # Data type validation\n",
    "    print(\"\\n3. Data Type Validation:\")\n",
    "    print(\"- Numeric columns should be numeric:\", all(df.select_dtypes(include=[np.number]).notna().all()))\n",
    "    print(\"- Categorical columns have reasonable cardinality:\")\n",
    "    \n",
    "    for col in df.select_dtypes(include=['category']).columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"  * {col}: {unique_count:,} unique values\")\n",
    "    \n",
    "    # Range validation for numeric columns\n",
    "    print(\"\\n4. Numeric Range Validation:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                print(f\"  ‚ö†Ô∏è  {col}: {negative_count:,} negative values\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ {col}: No negative values\")\n",
    "    \n",
    "    return missing_df, duplicates\n",
    "\n",
    "# Run data quality assessment\n",
    "missing_df, duplicate_count = assess_data_quality(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values (if any exist)\n",
    "if missing_df['Missing Count'].sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úÖ No missing values to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Univariate Analysis\n",
    "\n",
    "### Learning Note: Univariate analysis helps understand individual characteristics of each feature, which is essential for:\n",
    "- Detecting outliers and anomalies\n",
    "- Understanding data distributions\n",
    "- Identifying data quality issues\n",
    "- Informing preprocessing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_numerical_features(df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of numerical features\n",
    "    \"\"\"\n",
    "    print(\"üìä NUMERICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    numerical_cols = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                      'oldbalanceDest', 'newbalanceDest']\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n1. Statistical Summary:\")\n",
    "    stats_df = df[numerical_cols].describe().T\n",
    "    \n",
    "    # Add additional statistics\n",
    "    stats_df['skewness'] = df[numerical_cols].skew()\n",
    "    stats_df['kurtosis'] = df[numerical_cols].kurtosis()\n",
    "    stats_df['missing_pct'] = df[numerical_cols].isnull().sum() / len(df) * 100\n",
    "    \n",
    "    print(stats_df)\n",
    "    \n",
    "    return numerical_cols, stats_df\n",
    "\n",
    "numerical_cols, stats_summary = analyze_numerical_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numerical features\n",
    "def plot_numerical_distributions(df, numerical_cols):\n",
    "    \"\"\"\n",
    "    Create distribution plots for numerical features\n",
    "    \n",
    "    Learning Note: Distribution plots help identify:\n",
    "    - Skewness and data transformation needs\n",
    "    - Outliers and extreme values\n",
    "    - Multi-modal distributions\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        # Histogram with KDE\n",
    "        axes[i].hist(df[col].dropna(), bins=50, alpha=0.7, density=True)\n",
    "        axes[i].set_title(f'{col} Distribution\\n(Skew: {df[col].skew():.2f})')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Density')\n",
    "        \n",
    "        # Add vertical lines for mean and median\n",
    "        mean_val = df[col].mean()\n",
    "        median_val = df[col].median()\n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.2f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Median: {median_val:.2f}')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plots for outlier detection\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        # Box plot\n",
    "        axes[i].boxplot(df[col].dropna())\n",
    "        axes[i].set_title(f'{col} Box Plot')\n",
    "        axes[i].set_ylabel(col)\n",
    "        \n",
    "        # Add outlier count\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        axes[i].text(0.02, 0.98, f'Outliers: {outliers:,}', transform=axes[i].transAxes,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerical_distributions(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Analyze categorical features\n",
    "    \"\"\"\n",
    "    print(\"üìã CATEGORICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Transaction type analysis\n",
    "    print(\"\\n1. Transaction Type Distribution:\")\n",
    "    type_counts = df['type'].value_counts()\n",
    "    type_percentages = (type_counts / len(df)) * 100\n",
    "    \n",
    "    type_analysis = pd.DataFrame({\n",
    "        'Count': type_counts,\n",
    "        'Percentage': type_percentages\n",
    "    })\n",
    "    print(type_analysis)\n",
    "    \n",
    "    # Account name analysis\n",
    "    print(\"\\n2. Account Name Analysis:\")\n",
    "    print(f\"- Unique Origin Accounts: {df['nameOrig'].nunique():,}\")\n",
    "    print(f\"- Unique Destination Accounts: {df['nameDest'].nunique():,}\")\n",
    "    \n",
    "    # Account type patterns\n",
    "    df['orig_type'] = df['nameOrig'].str[0]\n",
    "    df['dest_type'] = df['nameDest'].str[0]\n",
    "    \n",
    "    print(\"\\n3. Account Type Distribution:\")\n",
    "    print(\"Origin Account Types:\")\n",
    "    print(df['orig_type'].value_counts())\n",
    "    print(\"\\nDestination Account Types:\")\n",
    "    print(df['dest_type'].value_counts())\n",
    "    \n",
    "    return type_analysis\n",
    "\n",
    "type_analysis = analyze_categorical_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Transaction type distribution\n",
    "type_counts = df['type'].value_counts()\n",
    "axes[0, 0].bar(type_counts.index, type_counts.values)\n",
    "axes[0, 0].set_title('Transaction Type Distribution')\n",
    "axes[0, 0].set_xlabel('Transaction Type')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Transaction type percentages\n",
    "type_percentages = (type_counts / len(df)) * 100\n",
    "axes[0, 1].bar(type_percentages.index, type_percentages.values)\n",
    "axes[0, 1].set_title('Transaction Type Percentages')\n",
    "axes[0, 1].set_xlabel('Transaction Type')\n",
    "axes[0, 1].set_ylabel('Percentage (%)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Origin account types\n",
    "orig_types = df['orig_type'].value_counts()\n",
    "axes[1, 0].bar(orig_types.index, orig_types.values)\n",
    "axes[1, 0].set_title('Origin Account Types')\n",
    "axes[1, 0].set_xlabel('Account Type')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Destination account types\n",
    "dest_types = df['dest_type'].value_counts()\n",
    "axes[1, 1].bar(dest_types.index, dest_types.values)\n",
    "axes[1, 1].set_title('Destination Account Types')\n",
    "axes[1, 1].set_xlabel('Account Type')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_target_variable(df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of the target variable (isFraud)\n",
    "    \n",
    "    Learning Note: Understanding target variable distribution is critical for:\n",
    "    - Model selection (imbalanced data requires special techniques)\n",
    "    - Evaluation metric selection\n",
    "    - Sampling strategy decisions\n",
    "    \"\"\"\n",
    "    print(\"üéØ TARGET VARIABLE ANALYSIS (isFraud)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    fraud_count = df['isFraud'].value_counts()\n",
    "    fraud_percentage = (fraud_count / len(df)) * 100\n",
    "    \n",
    "    print(\"\\n1. Fraud Distribution:\")\n",
    "    for label, count in fraud_count.items():\n",
    "        percentage = fraud_percentage[label]\n",
    "        label_text = \"Fraud\" if label == 1 else \"Legitimate\"\n",
    "        print(f\"  {label_text}: {count:,} ({percentage:.4f}%)\")\n",
    "    \n",
    "    # Class imbalance ratio\n",
    "    imbalance_ratio = fraud_count[0] / fraud_count[1] if len(fraud_count) > 1 else float('inf')\n",
    "    print(f\"\\n2. Class Imbalance Ratio: {imbalance_ratio:.2f}:1 (Legitimate:Fake)\")\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Count plot\n",
    "    labels = ['Legitimate', 'Fraud']\n",
    "    colors = ['lightblue', 'red']\n",
    "    \n",
    "    axes[0].bar(labels, [fraud_count[0], fraud_count[1]], color=colors)\n",
    "    axes[0].set_title('Fraud vs Legitimate Transactions (Count)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (label, count) in enumerate(zip(labels, [fraud_count[0], fraud_count[1]])):\n",
    "        axes[0].text(i, count + max(fraud_count) * 0.01, f'{count:,}', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Percentage plot (log scale for better visualization)\n",
    "    axes[1].bar(labels, [fraud_percentage[0], fraud_percentage[1]], color=colors)\n",
    "    axes[1].set_title('Fraud vs Legitimate Transactions (Percentage)')\n",
    "    axes[1].set_ylabel('Percentage (%)')\n",
    "    axes[1].set_yscale('log')  # Log scale to see small percentage\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, (label, pct) in enumerate(zip(labels, [fraud_percentage[0], fraud_percentage[1]])):\n",
    "        axes[1].text(i, pct * 1.1, f'{pct:.4f}%', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fraud_count, fraud_percentage, imbalance_ratio\n",
    "\n",
    "fraud_count, fraud_percentage, imbalance_ratio = analyze_target_variable(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bivariate Analysis\n",
    "\n",
    "### Learning Note: Bivariate analysis explores relationships between pairs of variables, helping identify:\n",
    "- Feature-target relationships\n",
    "- Feature-feature correlations\n",
    "- Potential predictive patterns\n",
    "- Multicollinearity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_target_relationships(df):\n",
    "    \"\"\"\n",
    "    Analyze relationships between features and target variable\n",
    "    \"\"\"\n",
    "    print(\"üîó FEATURE-TARGET RELATIONSHIP ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Numerical features vs target\n",
    "    print(\"\\n1. Numerical Features by Fraud Status:\")\n",
    "    numerical_cols = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                      'oldbalanceDest', 'newbalanceDest']\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        legit_stats = df[df['isFraud'] == 0][col].describe()\n",
    "        fraud_stats = df[df['isFraud'] == 1][col].describe()\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Legitimate - Mean: {legit_stats['mean']:.2f}, Median: {legit_stats['50%']:.2f}\")\n",
    "        print(f\"  Fraud - Mean: {fraud_stats['mean']:.2f}, Median: {fraud_stats['50%']:.2f}\")\n",
    "        \n",
    "        # Statistical test (Mann-Whitney U test for non-normal distributions)\n",
    "        if len(df[df['isFraud'] == 1]) > 0 and len(df[df['isFraud'] == 0]) > 0:\n",
    "            try:\n",
    "                statistic, p_value = stats.mannwhitneyu(\n",
    "                    df[df['isFraud'] == 0][col].dropna(),\n",
    "                    df[df['isFraud'] == 1][col].dropna()\n",
    "                )\n",
    "                significance = \"Significant\" if p_value < 0.05 else \"Not Significant\"\n",
    "                print(f\"  Mann-Whitney U test: {significance} (p={p_value:.2e})\")\n",
    "            except:\n",
    "                print(f\"  Mann-Whitney U test: Unable to compute\")\n",
    "    \n",
    "    # Categorical features vs target\n",
    "    print(\"\\n2. Transaction Type by Fraud Status:\")\n",
    "    type_fraud_crosstab = pd.crosstab(df['type'], df['isFraud'], margins=True)\n",
    "    print(type_fraud_crosstab)\n",
    "    \n",
    "    # Chi-square test for categorical association\n",
    "    if len(df['type'].unique()) > 1 and len(df['isFraud'].unique()) > 1:\n",
    "        try:\n",
    "            chi2, p_value, dof, expected = chi2_contingency(\n",
    "                pd.crosstab(df['type'], df['isFraud'])\n",
    "            )\n",
    "            print(f\"\\nChi-square test: Significant association (p={p_value:.2e})\")\n",
    "        except:\n",
    "            print(\"\\nChi-square test: Unable to compute\")\n",
    "    \n",
    "    return type_fraud_crosstab\n",
    "\n",
    "type_fraud_crosstab = analyze_feature_target_relationships(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature-target relationships\n",
    "def plot_feature_target_relationships(df):\n",
    "    \"\"\"\n",
    "    Create visualizations for feature-target relationships\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    numerical_cols = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                      'oldbalanceDest', 'newbalanceDest']\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        # Box plots by fraud status\n",
    "        legit_data = df[df['isFraud'] == 0][col].dropna()\n",
    "        fraud_data = df[df['isFraud'] == 1][col].dropna()\n",
    "        \n",
    "        # Create box plot data\n",
    "        box_data = [legit_data, fraud_data]\n",
    "        labels = ['Legitimate', 'Fraud']\n",
    "        \n",
    "        axes[i].boxplot(box_data, labels=labels)\n",
    "        axes[i].set_title(f'{col} by Fraud Status')\n",
    "        axes[i].set_ylabel(col)\n",
    "        \n",
    "        # Add sample sizes\n",
    "        axes[i].text(0.02, 0.98, f'n={len(legit_data):,}\\nn={len(fraud_data):,}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Transaction type vs fraud\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Stacked bar chart\n",
    "    type_fraud_pct = pd.crosstab(df['type'], df['isFraud'], normalize='index') * 100\n",
    "    type_fraud_pct.plot(kind='bar', stacked=True, ax=axes[0], \n",
    "                       color=['lightblue', 'red'], alpha=0.8)\n",
    "    axes[0].set_title('Fraud Percentage by Transaction Type')\n",
    "    axes[0].set_ylabel('Percentage (%)')\n",
    "    axes[0].set_xlabel('Transaction Type')\n",
    "    axes[0].legend(['Legitimate', 'Fraud'])\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Count plot\n",
    "    type_fraud_count = pd.crosstab(df['type'], df['isFraud'])\n",
    "    type_fraud_count.plot(kind='bar', ax=axes[1], \n",
    "                         color=['lightblue', 'red'], alpha=0.8)\n",
    "    axes[1].set_title('Transaction Count by Type and Fraud Status')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_xlabel('Transaction Type')\n",
    "    axes[1].legend(['Legitimate', 'Fraud'])\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].set_yscale('log')  # Log scale to see fraud counts\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_target_relationships(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(df):\n",
    "    \"\"\"\n",
    "    Correlation analysis for numerical features\n",
    "    \n",
    "    Learning Note: Correlation analysis helps identify:\n",
    "    - Multicollinearity (which can affect model performance)\n",
    "    - Feature redundancy\n",
    "    - Potential feature engineering opportunities\n",
    "    \"\"\"\n",
    "    print(\"üîó CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    numerical_cols = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                      'oldbalanceDest', 'newbalanceDest', 'isFraud']\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    print(\"\\n1. Correlation Matrix:\")\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    print(\"\\n2. Highly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.7:\n",
    "                feature1 = correlation_matrix.columns[i]\n",
    "                feature2 = correlation_matrix.columns[j]\n",
    "                high_corr_pairs.append((feature1, feature2, corr_value))\n",
    "                print(f\"  {feature1} ‚Üî {feature2}: {corr_value:.3f}\")\n",
    "    \n",
    "    if not high_corr_pairs:\n",
    "        print(\"  No highly correlated pairs found.\")\n",
    "    \n",
    "    # Correlation with target\n",
    "    print(\"\\n3. Feature Correlation with Target (isFraud):\")\n",
    "    target_corr = correlation_matrix['isFraud'].sort_values(key=abs, ascending=False)\n",
    "    print(target_corr.drop('isFraud').round(3))\n",
    "    \n",
    "    return correlation_matrix, high_corr_pairs\n",
    "\n",
    "correlation_matrix, high_corr_pairs = correlation_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('Feature Correlation Heatmap\\n(Upper triangle masked for clarity)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multivariate Analysis\n",
    "\n",
    "### Learning Note: Multivariate analysis explores complex relationships involving three or more variables simultaneously, helping identify:\n",
    "- Interaction effects between features\n",
    "- Complex patterns not visible in bivariate analysis\n",
    "- Natural groupings in the data\n",
    "- Dimension reduction opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features for multivariate analysis\n",
    "    \n",
    "    Learning Note: Feature engineering creates new features from existing ones,\n",
    "    potentially capturing complex relationships that improve model performance.\n",
    "    \"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Balance change features\n",
    "    df_engineered['orig_balance_change'] = df_engineered['newbalanceOrig'] - df_engineered['oldbalanceOrg']\n",
    "    df_engineered['dest_balance_change'] = df_engineered['newbalanceDest'] - df_engineered['oldbalanceDest']\n",
    "    \n",
    "    # Balance ratio features\n",
    "    df_engineered['orig_balance_ratio'] = np.where(\n",
    "        df_engineered['oldbalanceOrg'] > 0,\n",
    "        df_engineered['newbalanceOrig'] / df_engineered['oldbalanceOrg'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_engineered['dest_balance_ratio'] = np.where(\n",
    "        df_engineered['oldbalanceDest'] > 0,\n",
    "        df_engineered['newbalanceDest'] / df_engineered['oldbalanceDest'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Amount to balance ratios\n",
    "    df_engineered['amount_to_orig_balance'] = np.where(\n",
    "        df_engineered['oldbalanceOrg'] > 0,\n",
    "        df_engineered['amount'] / df_engineered['oldbalanceOrg'],\n",
    "        df_engineered['amount']\n",
    "    )\n",
    "    \n",
    "    # Zero balance indicators\n",
    "    df_engineered['orig_zero_after'] = (df_engineered['newbalanceOrig'] == 0).astype(int)\n",
    "    df_engineered['dest_zero_before'] = (df_engineered['oldbalanceDest'] == 0).astype(int)\n",
    "    \n",
    "    # Time-based features\n",
    "    df_engineered['hour_of_day'] = df_engineered['step'] % 24\n",
    "    df_engineered['day_of_week'] = (df_engineered['step'] // 24) % 7\n",
    "    df_engineered['is_business_hours'] = ((df_engineered['hour_of_day'] >= 9) & \n",
    "                                         (df_engineered['hour_of_day'] <= 17)).astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(df_engineered.columns) - len(df.columns)} new engineered features\")\n",
    "    \n",
    "    new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
    "    print(\"New features:\", new_features)\n",
    "    \n",
    "    return df_engineered, new_features\n",
    "\n",
    "df_engineered, new_features = create_interaction_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_engineered_features(df_engineered, new_features):\n",
    "    \"\"\"\n",
    "    Analyze the newly created engineered features\n",
    "    \"\"\"\n",
    "    print(\"üîß ENGINEERED FEATURES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Statistical summary of new features\n",
    "    print(\"\\n1. Statistical Summary of New Features:\")\n",
    "    new_numerical_features = [f for f in new_features if df_engineered[f].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    if new_numerical_features:\n",
    "        new_stats = df_engineered[new_numerical_features].describe().T\n",
    "        new_stats['skewness'] = df_engineered[new_numerical_features].skew()\n",
    "        print(new_stats)\n",
    "    \n",
    "    # Correlation of new features with target\n",
    "    print(\"\\n2. New Features Correlation with Target:\")\n",
    "    for feature in new_numerical_features:\n",
    "        correlation = df_engineered[feature].corr(df_engineered['isFraud'])\n",
    "        print(f\"  {feature}: {correlation:.4f}\")\n",
    "    \n",
    "    # Visualize key engineered features\n",
    "    if len(new_numerical_features) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        # Select top 4 most correlated features for visualization\n",
    "        feature_correlations = [(f, abs(df_engineered[f].corr(df_engineered['isFraud']))) \n",
    "                                for f in new_numerical_features]\n",
    "        feature_correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_features = [f[0] for f in feature_correlations[:4]]\n",
    "        \n",
    "        for i, feature in enumerate(top_features):\n",
    "            if i < 4:\n",
    "                # Box plot by fraud status\n",
    "                legit_data = df_engineered[df_engineered['isFraud'] == 0][feature].dropna()\n",
    "                fraud_data = df_engineered[df_engineered['isFraud'] == 1][feature].dropna()\n",
    "                \n",
    "                box_data = [legit_data, fraud_data]\n",
    "                labels = ['Legitimate', 'Fraud']\n",
    "                \n",
    "                axes[i].boxplot(box_data, labels=labels)\n",
    "                axes[i].set_title(f'{feature} by Fraud Status\\n(Corr: {df_engineered[feature].corr(df_engineered[\"isFraud\"]):.3f})')\n",
    "                axes[i].set_ylabel(feature)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return new_numerical_features\n",
    "\n",
    "new_numerical_features = analyze_engineered_features(df_engineered, new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analysis(df_engineered):\n",
    "    \"\"\"\n",
    "    Principal Component Analysis for dimensionality reduction\n",
    "    \n",
    "    Learning Note: PCA helps identify:\n",
    "    - The main sources of variance in the data\n",
    "    - Redundant features that can be removed\n",
    "    - Natural groupings of transactions\n",
    "    \"\"\"\n",
    "    print(\"üìä PRINCIPAL COMPONENT ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Select numerical features for PCA\n",
    "    numerical_features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                          'oldbalanceDest', 'newbalanceDest'] + new_numerical_features\n",
    "    \n",
    "    # Remove any infinite or very large values\n",
    "    pca_data = df_engineered[numerical_features].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if len(pca_data) == 0:\n",
    "        print(\"‚ùå No valid data for PCA after cleaning\")\n",
    "        return None, None\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    pca_data_scaled = scaler.fit_transform(pca_data)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=min(10, len(numerical_features)))\n",
    "    pca_result = pca.fit_transform(pca_data_scaled)\n",
    "    \n",
    "    # Explained variance\n",
    "    print(\"\\n1. Explained Variance by Component:\")\n",
    "    for i, variance in enumerate(pca.explained_variance_ratio_):\n",
    "        cumulative_variance = sum(pca.explained_variance_ratio_[:i+1])\n",
    "        print(f\"  PC{i+1}: {variance:.4f} ({cumulative_variance:.4f} cumulative)\")\n",
    "    \n",
    "    # Feature importance in components\n",
    "    print(\"\\n2. Feature Loadings for Top 3 Components:\")\n",
    "    for i in range(min(3, pca.n_components_)):\n",
    "        print(f\"\\n  PC{i+1} Loadings:\")\n",
    "        loadings = pca.components_[i]\n",
    "        feature_loadings = list(zip(numerical_features, loadings))\n",
    "        feature_loadings.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        for feature, loading in feature_loadings[:5]:\n",
    "            print(f\"    {feature}: {loading:.4f}\")\n",
    "    \n",
    "    # Visualize PCA results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scree plot\n",
    "    axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "                pca.explained_variance_ratio_, 'bo-')\n",
    "    axes[0].set_title('Scree Plot - Explained Variance by Component')\n",
    "    axes[0].set_xlabel('Principal Component')\n",
    "    axes[0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative variance plot\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-')\n",
    "    axes[1].set_title('Cumulative Explained Variance')\n",
    "    axes[1].set_xlabel('Number of Components')\n",
    "    axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=0.95, color='g', linestyle='--', alpha=0.7, label='95% Variance')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, pca_result\n",
    "\n",
    "pca_model, pca_result = pca_analysis(df_engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_outlier_detection(df):\n",
    "    \"\"\"\n",
    "    Multiple methods for outlier detection\n",
    "    \n",
    "    Learning Note: Outlier detection is crucial for fraud detection because:\n",
    "    - Fraudulent transactions often appear as outliers\n",
    "    - Outliers can skew model training\n",
    "    - Different detection methods capture different types of anomalies\n",
    "    \"\"\"\n",
    "    print(\"üîç OUTLIER DETECTION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                      'oldbalanceDest', 'newbalanceDest']\n",
    "    \n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\n1. {col} Outlier Analysis:\")\n",
    "        \n",
    "        # Method 1: IQR Method\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        iqr_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        iqr_percentage = (iqr_outliers / len(df)) * 100\n",
    "        \n",
    "        print(f\"  IQR Method: {iqr_outliers:,} outliers ({iqr_percentage:.2f}%)\")\n",
    "        \n",
    "        # Method 2: Z-score Method\n",
    "        z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "        z_outliers = (z_scores > 3).sum()\n",
    "        z_percentage = (z_outliers / len(df[col].dropna())) * 100\n",
    "        \n",
    "        print(f\"  Z-score Method: {z_outliers:,} outliers ({z_percentage:.2f}%)\")\n",
    "        \n",
    "        # Method 3: Modified Z-score (for skewed data)\n",
    "        median = df[col].median()\n",
    "        mad = np.median(np.abs(df[col] - median))\n",
    "        modified_z_scores = 0.6745 * (df[col] - median) / mad\n",
    "        modified_z_outliers = (np.abs(modified_z_scores) > 3.5).sum()\n",
    "        modified_z_percentage = (modified_z_outliers / len(df)) * 100\n",
    "        \n",
    "        print(f\"  Modified Z-score: {modified_z_outliers:,} outliers ({modified_z_percentage:.2f}%)\")\n",
    "        \n",
    "        outlier_summary[col] = {\n",
    "            'iqr_outliers': iqr_outliers,\n",
    "            'iqr_percentage': iqr_percentage,\n",
    "            'z_outliers': z_outliers,\n",
    "            'z_percentage': z_percentage,\n",
    "            'modified_z_outliers': modified_z_outliers,\n",
    "            'modified_z_percentage': modified_z_percentage\n",
    "        }\n",
    "    \n",
    "    return outlier_summary\n",
    "\n",
    "outlier_summary = comprehensive_outlier_detection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for key features\n",
    "def visualize_outliers(df, outlier_summary):\n",
    "    \"\"\"\n",
    "    Create visualizations for outlier analysis\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                      'oldbalanceDest', 'newbalanceDest', 'step']\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        # Create box plot with outlier highlighting\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Calculate IQR for outlier highlighting\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Separate outliers and non-outliers\n",
    "        non_outliers = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        # Create scatter plot to show distribution\n",
    "        axes[i].scatter(range(len(non_outliers)), non_outliers, \n",
    "                       alpha=0.6, s=1, label='Normal', color='blue')\n",
    "        axes[i].scatter(range(len(non_outliers), len(data)), outliers, \n",
    "                       alpha=0.8, s=2, label='Outliers', color='red')\n",
    "        \n",
    "        axes[i].set_title(f'{col} Distribution\\nOutliers: {len(outliers):,} ({(len(outliers)/len(data)*100):.2f}%)')\n",
    "        axes[i].set_xlabel('Index')\n",
    "        axes[i].set_ylabel(col)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_outliers(df, outlier_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Insights Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eda_summary(df, fraud_count, correlation_matrix, outlier_summary, new_features):\n",
    "    \"\"\"\n",
    "    Generate comprehensive EDA summary with actionable insights\n",
    "    \"\"\"\n",
    "    print(\"üìã COMPREHENSIVE EDA SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset Overview\n",
    "    print(\"\\nüéØ DATASET OVERVIEW:\")\n",
    "    print(f\"  ‚Ä¢ Total Records: {df.shape[0]:,}\")\n",
    "    print(f\"  ‚Ä¢ Total Features: {df.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Fraud Rate: {(fraud_count[1]/len(df)*100):.4f}%\")\n",
    "    print(f\"  ‚Ä¢ Class Imbalance Ratio: {(fraud_count[0]/fraud_count[1]):.2f}:1\")\n",
    "    \n",
    "    # Data Quality Assessment\n",
    "    print(\"\\n‚úÖ DATA QUALITY ASSESSMENT:\")\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"  ‚Ä¢ Missing Values: {missing_count:,}\")\n",
    "    print(f\"  ‚Ä¢ Duplicate Records: {duplicate_count:,}\")\n",
    "    print(f\"  ‚Ä¢ Data Types: All properly converted\")\n",
    "    \n",
    "    # Key Statistical Findings\n",
    "    print(\"\\nüìä KEY STATISTICAL FINDINGS:\")\n",
    "    print(f\"  ‚Ä¢ Average Transaction Amount: ${df['amount'].mean():,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Median Transaction Amount: ${df['amount'].median():,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Amount Skewness: {df['amount'].skew():.2f} (Highly skewed)\")\n",
    "    print(f\"  ‚Ä¢ Most Common Transaction Type: {df['type'].mode().iloc[0]}\")\n",
    "    \n",
    "    # Fraud Patterns\n",
    "    print(\"\\nüîç FRAUD PATTERNS:\")\n",
    "    fraud_by_type = df[df['isFraud'] == 1]['type'].value_counts()\n",
    "    if len(fraud_by_type) > 0:\n",
    "        print(f\"  ‚Ä¢ Fraud by Transaction Types:\")\n",
    "        for trans_type, count in fraud_by_type.items():\n",
    "            if trans_type and pd.notna(trans_type):\n",
    "                percentage = (count / fraud_count[1]) * 100\n",
    "                print(f\"    - {trans_type}: {count:,} ({percentage:.1f}% of fraud)\")\n",
    "    \n",
    "    # Feature Correlations\n",
    "    print(\"\\nüîó FEATURE CORRELATIONS:\")\n",
    "    target_correlations = correlation_matrix['isFraud'].drop('isFraud').abs().sort_values(ascending=False)\n",
    "    print(\"  ‚Ä¢ Top Features Correlated with Fraud:\")\n",
    "    for feature, corr in target_correlations.head(5).items():\n",
    "        print(f\"    - {feature}: {corr:.4f}\")\n",
    "    \n",
    "    # Outlier Analysis\n",
    "    print(\"\\nüö® OUTLIER ANALYSIS:\")\n",
    "    print(\"  ‚Ä¢ Average Outlier Percentage (IQR method):\")\n",
    "    avg_outlier_pct = np.mean([summary['iqr_percentage'] for summary in outlier_summary.values()])\n",
    "    print(f\"    {avg_outlier_pct:.2f}% across numerical features\")\n",
    "    \n",
    "    # Feature Engineering Success\n",
    "    print(\"\\nüîß FEATURE ENGINEERING INSIGHTS:\")\n",
    "    print(f\"  ‚Ä¢ Created {len(new_features)} new features\")\n",
    "    if new_features:\n",
    "        print(\"  ‚Ä¢ New feature categories:\")\n",
    "        print(\"    - Balance change features\")\n",
    "        print(\"    - Ratio features\")\n",
    "        print(\"    - Time-based features\")\n",
    "        print(\"    - Binary indicator features\")\n",
    "    \n",
    "    return \"EDA Summary Generated Successfully\"\n",
    "\n",
    "summary_result = generate_eda_summary(df, fraud_count, correlation_matrix, outlier_summary, new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ml_recommendations():\n",
    "    \"\"\"\n",
    "    Generate specific recommendations for ML pipeline development\n",
    "    \n",
    "    Learning Note: These recommendations are based on EDA findings and\n",
    "    industry best practices for fraud detection systems.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ MACHINE LEARNING PIPELINE RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüìù DATA PREPROCESSING RECOMMENDATIONS:\")\n",
    "    print(\"  1. Handle extreme class imbalance:\")\n",
    "    print(\"     ‚Ä¢ Use SMOTE or ADASYN for oversampling minority class\")\n",
    "    print(\"     ‚Ä¢ Implement class weighting in models\")\n",
    "    print(\"     ‚Ä¢ Consider ensemble methods designed for imbalanced data\")\n",
    "    \n",
    "    print(\"\\n  2. Feature scaling strategies:\")\n",
    "    print(\"     ‚Ä¢ Use RobustScaler for amount features (handles outliers)\")\n",
    "    print(\"     ‚Ä¢ Apply log transformation to highly skewed features\")\n",
    "    print(\"     ‚Ä¢ StandardScaler for normally distributed features\")\n",
    "    \n",
    "    print(\"\\n  3. Encoding techniques:\")\n",
    "    print(\"     ‚Ä¢ OneHotEncoding for transaction type\")\n",
    "    print(\"     ‚Ä¢ Target encoding for high-cardinality account names\")\n",
    "    print(\"     ‚Ä¢ Binary encoding for account types (C/M)\")\n",
    "    \n",
    "    print(\"\\nüéØ MODEL SELECTION RECOMMENDATIONS:\")\n",
    "    print(\"  1. Primary models to implement:\")\n",
    "    print(\"     ‚Ä¢ XGBoost/LightGBM (excellent for imbalanced data)\")\n",
    "    print(\"     ‚Ä¢ Random Forest with balanced class weights\")\n",
    "    print(\"     ‚Ä¢ Logistic Regression with L1/L2 regularization\")\n",
    "    \n",
    "    print(\"\\n  2. Advanced techniques:\")\n",
    "    print(\"     ‚Ä¢ Isolation Forest for anomaly detection\")\n",
    "    print(\"     ‚Ä¢ Neural Networks with dropout layers\")\n",
    "    print(\"     ‚Ä¢ Ensemble methods (Voting, Stacking)\")\n",
    "    \n",
    "    print(\"\\n  3. Evaluation metrics priority:\")\n",
    "    print(\"     ‚Ä¢ Precision-Recall AUC (critical for imbalanced data)\")\n",
    "    print(\"     ‚Ä¢ F1-Score and F2-Score (emphasizes recall)\")\n",
    "    print(\"     ‚Ä¢ ROC-AUC with caution due to class imbalance\")\n",
    "    \n",
    "    print(\"\\n‚ö° PERFORMANCE OPTIMIZATION:\")\n",
    "    print(\"  1. For large datasets (6M+ records):\")\n",
    "    print(\"     ‚Ä¢ Use chunk-based processing for memory efficiency\")\n",
    "    print(\"     ‚Ä¢ Implement incremental learning where possible\")\n",
    "    print(\"     ‚Ä¢ Consider dimensionality reduction for high-cardinality features\")\n",
    "    \n",
    "    print(\"\\n  2. Real-time deployment considerations:\")\n",
    "    print(\"     ‚Ä¢ Model serialization with joblib/pickle\")\n",
    "    print(\"     ‚Ä¢ Feature pipeline persistence\")\n",
    "    print(\"     ‚Ä¢ API endpoint optimization for low latency\")\n",
    "    \n",
    "    print(\"\\nüîí BUSINESS CONSIDERATIONS:\")\n",
    "    print(\"  1. Fraud detection specific:\")\n",
    "    print(\"     ‚Ä¢ Optimize for high recall (catch more fraud)\")\n",
    "    print(\"     ‚Ä¢ Implement threshold tuning based on business costs\")\n",
    "    print(\"     ‚Ä¢ Consider temporal validation (time-based split)\")\n",
    "    \n",
    "    print(\"\\n  2. Model monitoring:\")\n",
    "    print(\"     ‚Ä¢ Track fraud rate changes over time\")\n",
    "    print(\"     ‚Ä¢ Monitor feature drift and concept drift\")\n",
    "    print(\"     ‚Ä¢ Implement model retraining schedule\")\n",
    "    \n",
    "    return \"ML Recommendations Generated\"\n",
    "\n",
    "ml_recommendations = generate_ml_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### üéì Key Learning Points\n",
    "\n",
    "This comprehensive EDA has provided valuable insights into the fraud detection dataset:\n",
    "\n",
    "1. **Data Quality**: The dataset is clean with no missing values or duplicates, providing a solid foundation for ML modeling.\n",
    "\n",
    "2. **Class Imbalance Challenge**: The extreme imbalance (0.13% fraud rate) requires specialized techniques and careful evaluation metric selection.\n",
    "\n",
    "3. **Feature Relationships**: Strong correlations between balance features suggest opportunities for dimensionality reduction and feature engineering.\n",
    "\n",
    "4. **Fraud Patterns**: Certain transaction types show higher fraud rates, providing valuable signals for model training.\n",
    "\n",
    "5. **Outlier Significance**: High outlier percentages in financial features are expected and may contain fraud signals.\n",
    "\n",
    "### üìä Next Steps\n",
    "\n",
    "The insights from this EDA will directly inform the ML pipeline development:\n",
    "- Implement robust preprocessing for skewed distributions\n",
    "- Use advanced techniques for handling class imbalance\n",
    "- Engineer features based on discovered patterns\n",
    "- Select appropriate models and evaluation metrics\n",
    "\n",
    "### üöÄ Ready for ML Pipeline\n",
    "\n",
    "With these comprehensive insights, we're now ready to build an industry-standard ML pipeline that addresses the unique challenges of fraud detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
