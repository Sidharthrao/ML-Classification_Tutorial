{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud Detection ML Pipeline - End-to-End Workflow\n",
        "\n",
        "This notebook demonstrates the complete machine learning pipeline from data ingestion to model deployment.\n",
        "\n",
        "## Workflow Overview:\n",
        "1. **Configuration & Setup** - Project configuration and paths\n",
        "2. **Data Loading** - Extract data from SQLite database\n",
        "3. **Data Splitting** - Split into train/eval/test sets\n",
        "4. **Feature Engineering** - Create derived features\n",
        "5. **Preprocessing** - Data preprocessing pipeline\n",
        "6. **Model Training** - Train with class imbalance handling\n",
        "7. **Model Evaluation** - Comprehensive evaluation metrics\n",
        "8. **Prediction** - Batch and real-time predictions\n",
        "9. **API Deployment** - Flask API for production\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_score, recall_score, f1_score, accuracy_score,\n",
        "    precision_recall_curve, roc_curve, average_precision_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Models\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Model interpretation\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"SHAP not available\")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Optuna not available\")\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Configuration Setup\n",
        "**Source File**: `config/config.py`\n",
        "\n",
        "Setting up project paths, database connections, and hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION SETUP\n",
        "# Source: config/config.py\n",
        "# ============================================================================\n",
        "\n",
        "# Project root directory\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "\n",
        "# Database path - relative to project root\n",
        "DATABASE_PATH = PROJECT_ROOT.parent.parent.parent / \"Database.db\"\n",
        "\n",
        "# Data split configuration\n",
        "TRAIN_SIZE = 4_000_000  # First 4M records for training\n",
        "EVAL_SIZE = 1_000_000   # Next 1M records for evaluation\n",
        "\n",
        "# Directory paths\n",
        "DIRECTORIES = {\n",
        "    \"models\": PROJECT_ROOT / \"models\",\n",
        "    \"logs\": PROJECT_ROOT / \"logs\",\n",
        "    \"reports\": PROJECT_ROOT / \"reports\",\n",
        "    \"notebooks\": PROJECT_ROOT / \"notebooks\",\n",
        "    \"data\": PROJECT_ROOT / \"data\",\n",
        "}\n",
        "\n",
        "# Model file paths\n",
        "MODEL_PATHS = {\n",
        "    \"preprocessor\": DIRECTORIES[\"models\"] / \"preprocessor.pkl\",\n",
        "    \"model\": DIRECTORIES[\"models\"] / \"model.pkl\",\n",
        "    \"feature_names\": DIRECTORIES[\"models\"] / \"feature_names.pkl\",\n",
        "}\n",
        "\n",
        "# Column type mappings\n",
        "COLUMN_TYPES = {\n",
        "    \"step\": \"int64\",\n",
        "    \"type\": \"category\",\n",
        "    \"amount\": \"float64\",\n",
        "    \"nameOrig\": \"string\",\n",
        "    \"oldbalanceOrg\": \"float64\",\n",
        "    \"newbalanceOrig\": \"float64\",\n",
        "    \"nameDest\": \"string\",\n",
        "    \"oldbalanceDest\": \"float64\",\n",
        "    \"newbalanceDest\": \"float64\",\n",
        "    \"isFraud\": \"int64\",\n",
        "    \"isFlaggedFraud\": \"int64\",\n",
        "}\n",
        "\n",
        "# Database table name\n",
        "DB_TABLE_NAME = \"Fraud_detection\"\n",
        "\n",
        "# Model hyperparameters\n",
        "MODEL_CONFIG = {\n",
        "    \"primary_model\": \"xgboost\",\n",
        "    \"use_smote\": True,\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"cv_folds\": 5,\n",
        "    \"scoring_metric\": \"roc_auc\",\n",
        "}\n",
        "\n",
        "# XGBoost hyperparameters\n",
        "XGBOOST_PARAMS = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"max_depth\": 6,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"n_estimators\": 100,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"min_child_weight\": 3,\n",
        "    \"scale_pos_weight\": 100,  # For class imbalance\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "# LightGBM hyperparameters (alternative)\n",
        "LIGHTGBM_PARAMS = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"n_estimators\": 100,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"min_child_samples\": 20,\n",
        "    \"scale_pos_weight\": 100,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "# SMOTE configuration\n",
        "SMOTE_CONFIG = {\n",
        "    \"k_neighbors\": 5,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "# Feature engineering configuration\n",
        "FEATURE_CONFIG = {\n",
        "    \"use_account_frequency\": False,\n",
        "    \"use_time_features\": True,\n",
        "    \"use_balance_features\": True,\n",
        "    \"use_transaction_features\": True,\n",
        "}\n",
        "\n",
        "# Ensure directories exist\n",
        "for dir_path in DIRECTORIES.values():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded!\")\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"Database Path: {DATABASE_PATH}\")\n",
        "print(f\"Database exists: {DATABASE_PATH.exists()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Data Loading\n",
        "**Source File**: `src/data/data_loader.py`\n",
        "\n",
        "Functions to load data from SQLite database with chunking for large datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA LOADING FUNCTIONS\n",
        "# Source: src/data/data_loader.py\n",
        "# ============================================================================\n",
        "\n",
        "def load_data_from_db(\n",
        "    db_path=None,\n",
        "    table_name=None,\n",
        "    chunk_size=100000,\n",
        "    max_rows=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Load data from SQLite database with chunking for large datasets.\n",
        "    \"\"\"\n",
        "    if db_path is None:\n",
        "        db_path = DATABASE_PATH\n",
        "    if table_name is None:\n",
        "        table_name = DB_TABLE_NAME\n",
        "    \n",
        "    print(f\"Loading data from {db_path} table {table_name}\")\n",
        "    \n",
        "    if not db_path.exists():\n",
        "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
        "    \n",
        "    try:\n",
        "        conn = sqlite3.connect(str(db_path))\n",
        "        \n",
        "        # Get total row count\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
        "        total_rows = cursor.fetchone()[0]\n",
        "        print(f\"Total rows in table: {total_rows:,}\")\n",
        "        \n",
        "        # Determine how many rows to load\n",
        "        rows_to_load = min(total_rows, max_rows) if max_rows else total_rows\n",
        "        \n",
        "        # Load data in chunks\n",
        "        chunks = []\n",
        "        offset = 0\n",
        "        \n",
        "        while offset < rows_to_load:\n",
        "            current_chunk_size = min(chunk_size, rows_to_load - offset)\n",
        "            query = f\"SELECT * FROM {table_name} LIMIT {current_chunk_size} OFFSET {offset}\"\n",
        "            \n",
        "            chunk = pd.read_sql_query(query, conn)\n",
        "            chunks.append(chunk)\n",
        "            \n",
        "            offset += current_chunk_size\n",
        "            if offset % 500000 == 0:\n",
        "                print(f\"Loaded {offset:,}/{rows_to_load:,} rows\")\n",
        "        \n",
        "        conn.close()\n",
        "        \n",
        "        # Concatenate all chunks\n",
        "        df = pd.concat(chunks, ignore_index=True)\n",
        "        print(f\"Successfully loaded {len(df):,} rows\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def convert_column_types(df, column_types=None):\n",
        "    \"\"\"Convert column types according to schema.\"\"\"\n",
        "    if column_types is None:\n",
        "        column_types = COLUMN_TYPES\n",
        "    \n",
        "    print(\"Converting column types\")\n",
        "    df_converted = df.copy()\n",
        "    \n",
        "    for column, dtype in column_types.items():\n",
        "        if column in df_converted.columns:\n",
        "            try:\n",
        "                if dtype == \"float64\":\n",
        "                    df_converted[column] = pd.to_numeric(df_converted[column], errors=\"coerce\")\n",
        "                elif dtype == \"int64\":\n",
        "                    df_converted[column] = pd.to_numeric(df_converted[column], errors=\"coerce\").astype(\"Int64\")\n",
        "                elif dtype == \"category\":\n",
        "                    df_converted[column] = df_converted[column].astype(\"category\")\n",
        "                elif dtype == \"string\":\n",
        "                    df_converted[column] = df_converted[column].astype(\"string\")\n",
        "                else:\n",
        "                    df_converted[column] = df_converted[column].astype(dtype)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to convert {column} to {dtype}: {str(e)}\")\n",
        "    \n",
        "    return df_converted\n",
        "\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"Perform basic data validation checks.\"\"\"\n",
        "    print(\"Validating data\")\n",
        "    \n",
        "    # Check required columns\n",
        "    required_columns = [\"step\", \"type\", \"amount\", \"nameOrig\", \"oldbalanceOrg\", \n",
        "                        \"newbalanceOrig\", \"nameDest\", \"oldbalanceDest\", \n",
        "                        \"newbalanceDest\", \"isFraud\", \"isFlaggedFraud\"]\n",
        "    \n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "    \n",
        "    # Check for empty dataframe\n",
        "    if df.empty:\n",
        "        raise ValueError(\"DataFrame is empty\")\n",
        "    \n",
        "    # Log basic statistics\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
        "    print(f\"Fraud class distribution:\\n{df['isFraud'].value_counts()}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def load_and_prepare_data(\n",
        "    db_path=None,\n",
        "    table_name=None,\n",
        "    chunk_size=100000,\n",
        "    max_rows=None,\n",
        "    convert_types=True,\n",
        "    validate=True\n",
        "):\n",
        "    \"\"\"Complete data loading pipeline: load, convert types, and validate.\"\"\"\n",
        "    print(\"Starting data loading pipeline\")\n",
        "    \n",
        "    # Load data\n",
        "    df = load_data_from_db(db_path, table_name, chunk_size, max_rows)\n",
        "    \n",
        "    # Convert types\n",
        "    if convert_types:\n",
        "        df = convert_column_types(df)\n",
        "    \n",
        "    # Validate\n",
        "    if validate:\n",
        "        df = validate_data(df)\n",
        "    \n",
        "    print(\"Data loading pipeline completed successfully\")\n",
        "    return df\n",
        "\n",
        "# Load sample data for demonstration (use max_rows to limit for faster execution)\n",
        "print(\"Loading data sample...\")\n",
        "df = load_and_prepare_data(max_rows=10000)  # Limit for notebook demo\n",
        "print(f\"\\nData loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Data Splitting\n",
        "**Source File**: `src/data/data_splitter.py`\n",
        "\n",
        "Split dataset into train, evaluation, and test sets with proper stratification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA SPLITTING FUNCTIONS\n",
        "# Source: src/data/data_splitter.py\n",
        "# ============================================================================\n",
        "\n",
        "def split_data(\n",
        "    df,\n",
        "    train_size=TRAIN_SIZE,\n",
        "    eval_size=EVAL_SIZE,\n",
        "    random_state=42,\n",
        "    preserve_order=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Split dataset into train, evaluation, and test sets.\n",
        "    \"\"\"\n",
        "    print(f\"Splitting data: train={train_size:,}, eval={eval_size:,}\")\n",
        "    \n",
        "    total_rows = len(df)\n",
        "    print(f\"Total rows: {total_rows:,}\")\n",
        "    \n",
        "    if preserve_order and 'step' in df.columns:\n",
        "        # Sort by step to preserve temporal order\n",
        "        df_sorted = df.sort_values('step').reset_index(drop=True)\n",
        "        print(\"Preserving temporal order based on 'step' column\")\n",
        "    else:\n",
        "        df_sorted = df.copy()\n",
        "        if not preserve_order:\n",
        "            # Shuffle for random split\n",
        "            df_sorted = df_sorted.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "            print(\"Randomly shuffling data\")\n",
        "    \n",
        "    # Calculate split indices\n",
        "    train_end = min(train_size, total_rows)\n",
        "    eval_end = min(train_size + eval_size, total_rows)\n",
        "    \n",
        "    # Split data\n",
        "    train_df = df_sorted.iloc[:train_end].copy()\n",
        "    eval_df = df_sorted.iloc[train_end:eval_end].copy()\n",
        "    test_df = df_sorted.iloc[eval_end:].copy()\n",
        "    \n",
        "    print(f\"Train set: {len(train_df):,} rows\")\n",
        "    print(f\"Eval set: {len(eval_df):,} rows\")\n",
        "    print(f\"Test set: {len(test_df):,} rows\")\n",
        "    \n",
        "    # Log class distribution for each split\n",
        "    if 'isFraud' in train_df.columns:\n",
        "        print(f\"\\nTrain fraud distribution:\\n{train_df['isFraud'].value_counts()}\")\n",
        "        print(f\"Eval fraud distribution:\\n{eval_df['isFraud'].value_counts()}\")\n",
        "        print(f\"Test fraud distribution:\\n{test_df['isFraud'].value_counts()}\")\n",
        "    \n",
        "    return train_df, eval_df, test_df\n",
        "\n",
        "\n",
        "def split_features_target(df, target_column=\"isFraud\"):\n",
        "    \"\"\"\n",
        "    Split DataFrame into features and target.\n",
        "    \"\"\"\n",
        "    if target_column not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame\")\n",
        "    \n",
        "    # Drop target and other non-feature columns\n",
        "    columns_to_drop = [\n",
        "        target_column,\n",
        "        'isFlaggedFraud',  # Business flag, not a feature\n",
        "    ]\n",
        "    \n",
        "    # Keep only columns that exist\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "    \n",
        "    X = df.drop(columns=columns_to_drop).copy()\n",
        "    y = df[target_column].copy()\n",
        "    \n",
        "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
        "    print(f\"Feature columns: {list(X.columns)}\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Split the loaded data\n",
        "print(\"=\" * 60)\n",
        "print(\"SPLITTING DATA\")\n",
        "print(\"=\" * 60)\n",
        "train_df, eval_df, test_df = split_data(\n",
        "    df,\n",
        "    train_size=int(len(df) * 0.6),  # Adjusted for demo\n",
        "    eval_size=int(len(df) * 0.2),\n",
        "    preserve_order=True\n",
        ")\n",
        "\n",
        "# Split features and target\n",
        "X_train, y_train = split_features_target(train_df)\n",
        "X_eval, y_eval = split_features_target(eval_df)\n",
        "X_test, y_test = split_features_target(test_df)\n",
        "\n",
        "print(f\"\\nTraining set: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Evaluation set: X={X_eval.shape}, y={y_eval.shape}\")\n",
        "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Feature Engineering\n",
        "**Source File**: `src/preprocessing/feature_engineering.py`\n",
        "\n",
        "Create derived features from raw transaction data including balance, transaction, time, and account features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FEATURE ENGINEERING FUNCTIONS\n",
        "# Source: src/preprocessing/feature_engineering.py\n",
        "# ============================================================================\n",
        "\n",
        "def create_balance_features(df):\n",
        "    \"\"\"Create balance-related features.\"\"\"\n",
        "    df_features = df.copy()\n",
        "    \n",
        "    # Balance differences\n",
        "    df_features['balance_diff_orig'] = (\n",
        "        df_features['oldbalanceOrg'] - df_features['newbalanceOrig']\n",
        "    )\n",
        "    df_features['balance_diff_dest'] = (\n",
        "        df_features['newbalanceDest'] - df_features['oldbalanceDest']\n",
        "    )\n",
        "    \n",
        "    # Zero balance flags\n",
        "    df_features['balance_orig_zero'] = (df_features['oldbalanceOrg'] == 0).astype(int)\n",
        "    df_features['balance_dest_zero'] = (df_features['oldbalanceDest'] == 0).astype(int)\n",
        "    \n",
        "    # Zero balance after transaction\n",
        "    df_features['zero_balance_after_transaction'] = (\n",
        "        df_features['newbalanceOrig'] == 0\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Balance ratios\n",
        "    df_features['balance_orig_ratio'] = np.where(\n",
        "        df_features['oldbalanceOrg'] > 0,\n",
        "        df_features['amount'] / df_features['oldbalanceOrg'],\n",
        "        0\n",
        "    )\n",
        "    \n",
        "    df_features['balance_dest_ratio'] = np.where(\n",
        "        df_features['oldbalanceDest'] > 0,\n",
        "        df_features['amount'] / (df_features['oldbalanceDest'] + 1),\n",
        "        0\n",
        "    )\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "\n",
        "def create_transaction_features(df):\n",
        "    \"\"\"Create transaction-related features.\"\"\"\n",
        "    df_features = df.copy()\n",
        "    \n",
        "    # Log amount (handle zero and negative)\n",
        "    df_features['amount_log'] = np.log1p(df_features['amount'].clip(lower=0))\n",
        "    \n",
        "    # Amount per original balance\n",
        "    df_features['amount_per_balance_orig'] = (\n",
        "        df_features['amount'] / (df_features['oldbalanceOrg'] + 1)\n",
        "    )\n",
        "    \n",
        "    # Check if transaction empties origin account\n",
        "    df_features['empties_origin'] = (\n",
        "        (df_features['oldbalanceOrg'] > 0) & \n",
        "        (df_features['newbalanceOrig'] == 0)\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Check if transaction creates new destination balance\n",
        "    df_features['creates_dest_balance'] = (\n",
        "        (df_features['oldbalanceDest'] == 0) & \n",
        "        (df_features['newbalanceDest'] > 0)\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Amount categories (buckets)\n",
        "    df_features['amount_category'] = pd.cut(\n",
        "        df_features['amount'],\n",
        "        bins=[0, 100, 1000, 10000, 100000, float('inf')],\n",
        "        labels=['very_small', 'small', 'medium', 'large', 'very_large'],\n",
        "        include_lowest=True\n",
        "    )\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "\n",
        "def create_time_features(df):\n",
        "    \"\"\"Create time-related features from step column.\"\"\"\n",
        "    df_features = df.copy()\n",
        "    \n",
        "    if 'step' not in df_features.columns:\n",
        "        print(\"Warning: 'step' column not found, skipping time features\")\n",
        "        return df_features\n",
        "    \n",
        "    # Hour of day (step represents hours)\n",
        "    df_features['hour_of_day'] = df_features['step'] % 24\n",
        "    \n",
        "    # Day of week (assuming step 0 is start of week)\n",
        "    df_features['day_of_week'] = (df_features['step'] // 24) % 7\n",
        "    \n",
        "    # Is weekend\n",
        "    df_features['is_weekend'] = (\n",
        "        (df_features['day_of_week'] == 5) | (df_features['day_of_week'] == 6)\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Is business hours (9-17)\n",
        "    df_features['is_business_hours'] = (\n",
        "        (df_features['hour_of_day'] >= 9) & (df_features['hour_of_day'] < 17)\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Is night (22-6)\n",
        "    df_features['is_night'] = (\n",
        "        (df_features['hour_of_day'] >= 22) | (df_features['hour_of_day'] < 6)\n",
        "    ).astype(int)\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "\n",
        "def create_account_features(df, use_frequency=False):\n",
        "    \"\"\"Create account-related features.\"\"\"\n",
        "    df_features = df.copy()\n",
        "    \n",
        "    # Same account transfer flag\n",
        "    df_features['same_account_transfer'] = (\n",
        "        df_features['nameOrig'] == df_features['nameDest']\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Account name prefixes (C = customer, M = merchant)\n",
        "    df_features['orig_is_customer'] = (\n",
        "        df_features['nameOrig'].str.startswith('C', na=False)\n",
        "    ).astype(int)\n",
        "    df_features['dest_is_customer'] = (\n",
        "        df_features['nameDest'].str.startswith('C', na=False)\n",
        "    ).astype(int)\n",
        "    \n",
        "    if use_frequency:\n",
        "        print(\"Computing account frequency features (this may take time)\")\n",
        "        # Frequency of origin account\n",
        "        orig_counts = df_features['nameOrig'].value_counts()\n",
        "        df_features['orig_account_frequency'] = df_features['nameOrig'].map(orig_counts)\n",
        "        \n",
        "        # Frequency of destination account\n",
        "        dest_counts = df_features['nameDest'].value_counts()\n",
        "        df_features['dest_account_frequency'] = df_features['nameDest'].map(dest_counts)\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "\n",
        "def create_all_features(df, feature_config=None):\n",
        "    \"\"\"Create all engineered features.\"\"\"\n",
        "    if feature_config is None:\n",
        "        feature_config = FEATURE_CONFIG\n",
        "    \n",
        "    print(\"Starting feature engineering pipeline\")\n",
        "    df_features = df.copy()\n",
        "    \n",
        "    # Balance features\n",
        "    if feature_config.get(\"use_balance_features\", True):\n",
        "        df_features = create_balance_features(df_features)\n",
        "    \n",
        "    # Transaction features\n",
        "    if feature_config.get(\"use_transaction_features\", True):\n",
        "        df_features = create_transaction_features(df_features)\n",
        "    \n",
        "    # Time features\n",
        "    if feature_config.get(\"use_time_features\", True):\n",
        "        df_features = create_time_features(df_features)\n",
        "    \n",
        "    # Account features\n",
        "    df_features = create_account_features(\n",
        "        df_features,\n",
        "        use_frequency=feature_config.get(\"use_account_frequency\", False)\n",
        "    )\n",
        "    \n",
        "    print(f\"Feature engineering complete. Shape: {df_features.shape}\")\n",
        "    new_features = [col for col in df_features.columns if col not in df.columns]\n",
        "    print(f\"New feature columns: {len(new_features)} features created\")\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "# Apply feature engineering to training data\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 60)\n",
        "X_train_features = create_all_features(X_train)\n",
        "print(f\"\\nOriginal columns: {len(X_train.columns)}\")\n",
        "print(f\"Features after engineering: {len(X_train_features.columns)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Preprocessing Pipeline\n",
        "**Source File**: `src/preprocessing/preprocessor.py`\n",
        "\n",
        "Sklearn-compatible preprocessing pipeline that handles feature engineering, encoding, scaling, and imputation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PREPROCESSING PIPELINE CLASS\n",
        "# Source: src/preprocessing/preprocessor.py\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FraudDetectionPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Sklearn-compatible preprocessor for fraud detection.\n",
        "    Handles feature engineering, encoding, scaling, and imputation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_config=None,\n",
        "        categorical_columns=None,\n",
        "        numerical_columns=None,\n",
        "        use_one_hot=True,\n",
        "        use_scaling=True\n",
        "    ):\n",
        "        \"\"\"Initialize preprocessor.\"\"\"\n",
        "        self.feature_config = feature_config or FEATURE_CONFIG\n",
        "        self.categorical_columns = categorical_columns or []\n",
        "        self.numerical_columns = numerical_columns or []\n",
        "        self.use_one_hot = use_one_hot\n",
        "        self.use_scaling = use_scaling\n",
        "        \n",
        "        # Transformers\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
        "        self.scaler = StandardScaler()\n",
        "        self.imputer = SimpleImputer(strategy='median')\n",
        "        \n",
        "        # Feature names after transformation\n",
        "        self.feature_names_ = None\n",
        "        self.is_fitted_ = False\n",
        "        \n",
        "    def _identify_columns(self, X):\n",
        "        \"\"\"Identify categorical and numerical columns if not provided.\"\"\"\n",
        "        if not self.categorical_columns and not self.numerical_columns:\n",
        "            for col in X.columns:\n",
        "                if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                    if col not in self.categorical_columns:\n",
        "                        self.categorical_columns.append(col)\n",
        "                elif X[col].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
        "                    if col not in self.numerical_columns:\n",
        "                        self.numerical_columns.append(col)\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the preprocessor on training data.\"\"\"\n",
        "        print(\"Fitting preprocessor\")\n",
        "        \n",
        "        # Create features\n",
        "        X_features = create_all_features(X, self.feature_config)\n",
        "        \n",
        "        # Identify columns if not already done\n",
        "        self._identify_columns(X_features)\n",
        "        \n",
        "        # Handle missing values\n",
        "        X_imputed = X_features.copy()\n",
        "        if self.numerical_columns:\n",
        "            numerical_data = X_imputed[self.numerical_columns].select_dtypes(include=[np.number])\n",
        "            self.imputer.fit(numerical_data)\n",
        "        \n",
        "        # Fit encoders\n",
        "        if self.categorical_columns:\n",
        "            categorical_data = X_imputed[self.categorical_columns]\n",
        "            \n",
        "            if self.use_one_hot:\n",
        "                self.one_hot_encoder.fit(categorical_data)\n",
        "            else:\n",
        "                if len(self.categorical_columns) == 1:\n",
        "                    self.label_encoder.fit(categorical_data[self.categorical_columns[0]])\n",
        "        \n",
        "        # Fit scaler\n",
        "        if self.use_scaling and self.numerical_columns:\n",
        "            numerical_data = X_imputed[self.numerical_columns].select_dtypes(include=[np.number])\n",
        "            if len(numerical_data.columns) > 0:\n",
        "                self.scaler.fit(self.imputer.transform(numerical_data))\n",
        "        \n",
        "        # Store feature names\n",
        "        self.feature_names_ = list(X_features.columns)\n",
        "        self.is_fitted_ = True\n",
        "        \n",
        "        print(f\"Preprocessor fitted. Output features: {len(self.feature_names_)}\")\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data using fitted preprocessor.\"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
        "        \n",
        "        # Create features\n",
        "        X_features = create_all_features(X, self.feature_config)\n",
        "        \n",
        "        # Handle missing values\n",
        "        X_processed = X_features.copy()\n",
        "        \n",
        "        # Impute numerical columns\n",
        "        if self.numerical_columns:\n",
        "            numerical_data = X_processed[self.numerical_columns].select_dtypes(include=[np.number])\n",
        "            if len(numerical_data.columns) > 0:\n",
        "                numerical_imputed = self.imputer.transform(numerical_data)\n",
        "                \n",
        "                # Scale if enabled\n",
        "                if self.use_scaling:\n",
        "                    numerical_scaled = self.scaler.transform(numerical_imputed)\n",
        "                else:\n",
        "                    numerical_scaled = numerical_imputed\n",
        "                \n",
        "                # Update DataFrame\n",
        "                for i, col in enumerate(numerical_data.columns):\n",
        "                    X_processed[col] = numerical_scaled[:, i]\n",
        "        \n",
        "        # Encode categorical columns\n",
        "        if self.categorical_columns:\n",
        "            categorical_data = X_processed[self.categorical_columns]\n",
        "            \n",
        "            if self.use_one_hot:\n",
        "                categorical_encoded = self.one_hot_encoder.transform(categorical_data)\n",
        "                categorical_encoded_df = pd.DataFrame(\n",
        "                    categorical_encoded,\n",
        "                    columns=self.one_hot_encoder.get_feature_names_out(self.categorical_columns),\n",
        "                    index=X_processed.index\n",
        "                )\n",
        "                X_processed = X_processed.drop(columns=self.categorical_columns)\n",
        "                X_processed = pd.concat([X_processed, categorical_encoded_df], axis=1)\n",
        "            else:\n",
        "                if len(self.categorical_columns) == 1:\n",
        "                    X_processed[self.categorical_columns[0]] = self.label_encoder.transform(\n",
        "                        categorical_data[self.categorical_columns[0]]\n",
        "                    )\n",
        "        \n",
        "        return X_processed\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform in one step.\"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "# Fit and transform the training data\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "preprocessor = FraudDetectionPreprocessor()\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_eval_transformed = preprocessor.transform(X_eval)\n",
        "\n",
        "print(f\"\\nOriginal shape: {X_train.shape}\")\n",
        "print(f\"Transformed shape: {X_train_transformed.shape}\")\n",
        "print(f\"Number of features: {len(X_train_transformed.columns)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL TRAINING CLASS\n",
        "# Source: src/models/model_trainer.py\n",
        "# ============================================================================\n",
        "\n",
        "class FraudDetectionModelTrainer:\n",
        "    \"\"\"Model trainer for fraud detection with class imbalance handling.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type=\"xgboost\",\n",
        "        use_smote=True,\n",
        "        smote_config=None,\n",
        "        random_state=42\n",
        "    ):\n",
        "        \"\"\"Initialize model trainer.\"\"\"\n",
        "        self.model_type = model_type.lower()\n",
        "        self.use_smote = use_smote\n",
        "        self.smote_config = smote_config or SMOTE_CONFIG\n",
        "        self.random_state = random_state\n",
        "        \n",
        "        self.model = None\n",
        "        self.smote = None\n",
        "        self.best_params_ = None\n",
        "        \n",
        "        print(f\"Initialized trainer with model_type={model_type}, use_smote={use_smote}\")\n",
        "    \n",
        "    def _create_model(self, params=None):\n",
        "        \"\"\"Create model instance based on model_type.\"\"\"\n",
        "        if params is None:\n",
        "            params = {}\n",
        "        \n",
        "        if self.model_type == \"xgboost\":\n",
        "            default_params = XGBOOST_PARAMS.copy()\n",
        "            default_params.update(params)\n",
        "            default_params['random_state'] = self.random_state\n",
        "            return XGBClassifier(**default_params)\n",
        "        \n",
        "        elif self.model_type == \"lightgbm\":\n",
        "            default_params = LIGHTGBM_PARAMS.copy()\n",
        "            default_params.update(params)\n",
        "            default_params['random_state'] = self.random_state\n",
        "            return LGBMClassifier(**default_params)\n",
        "        \n",
        "        elif self.model_type == \"random_forest\":\n",
        "            rf_params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 20,\n",
        "                'min_samples_split': 5,\n",
        "                'min_samples_leaf': 2,\n",
        "                'class_weight': 'balanced',\n",
        "                'random_state': self.random_state,\n",
        "                **params\n",
        "            }\n",
        "            return RandomForestClassifier(**rf_params)\n",
        "        \n",
        "        elif self.model_type == \"logistic\":\n",
        "            lr_params = {\n",
        "                'max_iter': 1000,\n",
        "                'class_weight': 'balanced',\n",
        "                'random_state': self.random_state,\n",
        "                **params\n",
        "            }\n",
        "            return LogisticRegression(**lr_params)\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model_type: {self.model_type}\")\n",
        "    \n",
        "    def _create_smote(self):\n",
        "        \"\"\"Create SMOTE instance.\"\"\"\n",
        "        k_neighbors = self.smote_config.get('k_neighbors', 5)\n",
        "        random_state = self.smote_config.get('random_state', self.random_state)\n",
        "        return SMOTE(\n",
        "            k_neighbors=k_neighbors,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    \n",
        "    def fit(self, X, y, tune_hyperparameters=False, n_trials=20):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        print(f\"Training {self.model_type} model\")\n",
        "        print(f\"Training data shape: {X.shape}, Target distribution: {y.value_counts().to_dict()}\")\n",
        "        \n",
        "        # Handle class imbalance with SMOTE\n",
        "        if self.use_smote:\n",
        "            print(\"Applying SMOTE for class imbalance\")\n",
        "            self.smote = self._create_smote()\n",
        "            \n",
        "            min_class_count = y.value_counts().min()\n",
        "            k_neighbors = self.smote_config.get('k_neighbors', 5)\n",
        "            \n",
        "            if min_class_count <= k_neighbors:\n",
        "                print(f\"Not enough samples for SMOTE. Using class_weight instead.\")\n",
        "                self.use_smote = False\n",
        "            else:\n",
        "                try:\n",
        "                    X_resampled, y_resampled = self.smote.fit_resample(X, y)\n",
        "                    print(f\"After SMOTE: {X_resampled.shape}, Target distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
        "                    X, y = X_resampled, y_resampled\n",
        "                except Exception as e:\n",
        "                    print(f\"SMOTE failed: {str(e)}. Using class_weight instead.\")\n",
        "                    self.use_smote = False\n",
        "        \n",
        "        # Create and train model\n",
        "        self.model = self._create_model()\n",
        "        \n",
        "        print(\"Training model...\")\n",
        "        self.model.fit(X, y)\n",
        "        print(\"Model training completed\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        return self.model.predict(X)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities.\"\"\"\n",
        "        return self.model.predict_proba(X)\n",
        "    \n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save model to file.\"\"\"\n",
        "        print(f\"Saving model to {filepath}\")\n",
        "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "        joblib.dump(self.model, filepath)\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, filepath):\n",
        "        \"\"\"Load model from file.\"\"\"\n",
        "        print(f\"Loading model from {filepath}\")\n",
        "        return joblib.load(filepath)\n",
        "\n",
        "# Train the model\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "trainer = FraudDetectionModelTrainer(\n",
        "    model_type=MODEL_CONFIG.get(\"primary_model\", \"xgboost\"),\n",
        "    use_smote=MODEL_CONFIG.get(\"use_smote\", True),\n",
        "    random_state=MODEL_CONFIG.get(\"random_state\", 42)\n",
        ")\n",
        "\n",
        "trainer.fit(X_train_transformed, y_train, tune_hyperparameters=False)\n",
        "print(\"\\nModel trained successfully!\")\n",
        "\n",
        "# Save model\n",
        "trainer.save(MODEL_PATHS[\"model\"])\n",
        "print(f\"Model saved to {MODEL_PATHS['model']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Model Evaluation\n",
        "**Source File**: `src/evaluation/model_evaluator.py`\n",
        "\n",
        "Comprehensive evaluation with metrics, visualizations, and report generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL EVALUATION CLASS\n",
        "# Source: src/evaluation/model_evaluator.py\n",
        "# ============================================================================\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation with metrics and visualizations.\"\"\"\n",
        "    \n",
        "    def __init__(self, model, preprocessor=None, optimize_threshold=True, target_recall=0.90):\n",
        "        \"\"\"Initialize evaluator.\"\"\"\n",
        "        self.model = model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.optimize_threshold = optimize_threshold\n",
        "        self.target_recall = target_recall\n",
        "        self.optimal_threshold_ = None\n",
        "        self.metrics_ = {}\n",
        "    \n",
        "    def evaluate(self, X, y_true, save_plots=True):\n",
        "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "        print(\"Starting model evaluation\")\n",
        "        \n",
        "        # Get predictions and probabilities\n",
        "        y_pred = self.model.predict(X)\n",
        "        y_proba = self.model.predict_proba(X)\n",
        "        \n",
        "        # If binary classification, get probabilities for positive class\n",
        "        if y_proba.shape[1] == 2:\n",
        "            y_proba_positive = y_proba[:, 1]\n",
        "        else:\n",
        "            y_proba_positive = y_proba[:, -1]\n",
        "        \n",
        "        # Calculate metrics with default threshold (0.5)\n",
        "        metrics = self._calculate_metrics(y_true, y_pred, y_proba_positive)\n",
        "        self.metrics_ = metrics\n",
        "        \n",
        "        # Optimize threshold if requested\n",
        "        if self.optimize_threshold:\n",
        "            optimal_threshold = self._optimize_threshold(y_true, y_proba_positive)\n",
        "            self.optimal_threshold_ = optimal_threshold\n",
        "            \n",
        "            y_pred_optimal = (y_proba_positive >= optimal_threshold).astype(int)\n",
        "            metrics_optimal = self._calculate_metrics(y_true, y_pred_optimal, y_proba_positive)\n",
        "            metrics['optimal_threshold'] = optimal_threshold\n",
        "            metrics['metrics_at_optimal_threshold'] = metrics_optimal\n",
        "        \n",
        "        # Generate visualizations\n",
        "        if save_plots:\n",
        "            self._generate_plots(X, y_true, y_pred, y_proba_positive, metrics)\n",
        "        \n",
        "        print(\"Model evaluation completed\")\n",
        "        return metrics\n",
        "    \n",
        "    def _calculate_metrics(self, y_true, y_pred, y_proba):\n",
        "        \"\"\"Calculate all evaluation metrics.\"\"\"\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(y_true, y_proba),\n",
        "            'pr_auc': average_precision_score(y_true, y_proba),\n",
        "        }\n",
        "        return metrics\n",
        "    \n",
        "    def _optimize_threshold(self, y_true, y_proba):\n",
        "        \"\"\"Optimize classification threshold based on target recall.\"\"\"\n",
        "        precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_proba)\n",
        "        target_idx = np.argmax(recall_vals >= self.target_recall)\n",
        "        \n",
        "        if target_idx > 0:\n",
        "            optimal_threshold = thresholds[target_idx - 1]\n",
        "        else:\n",
        "            optimal_threshold = 0.5\n",
        "        \n",
        "        return optimal_threshold\n",
        "    \n",
        "    def _generate_plots(self, X, y_true, y_pred, y_proba, metrics):\n",
        "        \"\"\"Generate evaluation plots.\"\"\"\n",
        "        reports_dir = DIRECTORIES[\"reports\"]\n",
        "        reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # 1. Confusion Matrix\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "        axes[0].set_title('Confusion Matrix')\n",
        "        axes[0].set_xlabel('Predicted')\n",
        "        axes[0].set_ylabel('Actual')\n",
        "        \n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1])\n",
        "        axes[1].set_title('Normalized Confusion Matrix')\n",
        "        axes[1].set_xlabel('Predicted')\n",
        "        axes[1].set_ylabel('Actual')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(reports_dir / \"confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        # 2. ROC Curve\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(reports_dir / \"roc_curve.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        # 3. Precision-Recall Curve\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(recall, precision, label=f'PR Curve (AUC = {metrics[\"pr_auc\"]:.4f})')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(reports_dir / \"pr_curve.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        # 4. Feature Importance\n",
        "        if hasattr(self.model, 'feature_importances_'):\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': self.model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False).head(20)\n",
        "            \n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.barplot(data=feature_importance, y='feature', x='importance')\n",
        "            plt.xlabel('Importance')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.title('Top 20 Feature Importance')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(reports_dir / \"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "evaluator = ModelEvaluator(\n",
        "    model=trainer.model,\n",
        "    preprocessor=preprocessor,\n",
        "    optimize_threshold=True,\n",
        "    target_recall=0.90\n",
        ")\n",
        "\n",
        "metrics = evaluator.evaluate(X_eval_transformed, y_eval, save_plots=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
        "print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
        "print(f\"PR-AUC: {metrics['pr_auc']:.4f}\")\n",
        "\n",
        "if 'optimal_threshold' in metrics:\n",
        "    print(f\"\\nOptimal Threshold: {metrics['optimal_threshold']:.4f}\")\n",
        "    print(\"\\nMetrics at Optimal Threshold:\")\n",
        "    opt_metrics = metrics['metrics_at_optimal_threshold']\n",
        "    print(f\"  Accuracy: {opt_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {opt_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {opt_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1-Score: {opt_metrics['f1_score']:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Predictions\n",
        "**Source File**: `scripts/predict.py`\n",
        "\n",
        "Make predictions on new data using the trained model and preprocessor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PREDICTION FUNCTIONS\n",
        "# Source: scripts/predict.py\n",
        "# ============================================================================\n",
        "\n",
        "def predict_batch(X, model, preprocessor, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Make batch predictions on new data.\n",
        "    \n",
        "    Args:\n",
        "        X: Feature DataFrame\n",
        "        model: Trained model\n",
        "        preprocessor: Fitted preprocessor\n",
        "        threshold: Classification threshold\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with predictions and probabilities\n",
        "    \"\"\"\n",
        "    print(\"Making predictions...\")\n",
        "    \n",
        "    # Preprocess\n",
        "    X_transformed = preprocessor.transform(X)\n",
        "    \n",
        "    # Predict\n",
        "    predictions = model.predict(X_transformed)\n",
        "    probabilities = model.predict_proba(X_transformed)\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results = X.copy()\n",
        "    results['predicted_fraud'] = predictions\n",
        "    results['fraud_probability'] = probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0]\n",
        "    results['is_fraud'] = (results['fraud_probability'] >= threshold).astype(int)\n",
        "    \n",
        "    print(f\"Predictions complete: {len(results)} transactions, {results['is_fraud'].sum()} flagged as fraud\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Make predictions on test set\n",
        "print(\"=\" * 60)\n",
        "print(\"MAKING PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Apply feature engineering to test set\n",
        "X_test_features = create_all_features(X_test)\n",
        "X_test_transformed = preprocessor.transform(X_test_features)\n",
        "\n",
        "# Predictions\n",
        "test_predictions = predict_batch(X_test, trainer.model, preprocessor, threshold=0.5)\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "print(test_predictions[['step', 'type', 'amount', 'fraud_probability', 'is_fraud']].head(10))\n",
        "\n",
        "print(\"\\nPrediction Summary:\")\n",
        "print(f\"Total transactions: {len(test_predictions)}\")\n",
        "print(f\"Predicted fraud: {test_predictions['is_fraud'].sum()}\")\n",
        "print(f\"Fraud rate: {test_predictions['is_fraud'].mean()*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Model Saving and Loading\n",
        "**Source Files**: `scripts/train_model.py`, `scripts/predict.py`\n",
        "\n",
        "Save and load model artifacts for production use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAVE MODEL ARTIFACTS\n",
        "# Source: scripts/train_model.py\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING MODEL ARTIFACTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save preprocessor\n",
        "preprocessor.save(MODEL_PATHS[\"preprocessor\"])\n",
        "print(f\" Preprocessor saved to {MODEL_PATHS['preprocessor']}\")\n",
        "\n",
        "# Save model\n",
        "trainer.save(MODEL_PATHS[\"model\"])\n",
        "print(f\" Model saved to {MODEL_PATHS['model']}\")\n",
        "\n",
        "# Save feature names\n",
        "joblib.dump(preprocessor.feature_names_, MODEL_PATHS[\"feature_names\"])\n",
        "print(f\" Feature names saved to {MODEL_PATHS['feature_names']}\")\n",
        "\n",
        "print(\"\\nAll artifacts saved successfully!\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD MODEL ARTIFACTS (Example for production use)\n",
        "# Source: scripts/predict.py\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING MODEL ARTIFACTS (Example)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load preprocessor\n",
        "loaded_preprocessor = FraudDetectionPreprocessor.load(MODEL_PATHS[\"preprocessor\"])\n",
        "print(\" Preprocessor loaded\")\n",
        "\n",
        "# Load model\n",
        "loaded_model = FraudDetectionModelTrainer.load(MODEL_PATHS[\"model\"])\n",
        "print(\" Model loaded\")\n",
        "\n",
        "# Load feature names\n",
        "loaded_feature_names = joblib.load(MODEL_PATHS[\"feature_names\"])\n",
        "print(f\" Feature names loaded ({len(loaded_feature_names)} features)\")\n",
        "\n",
        "print(\"\\nAll artifacts loaded successfully!\")\n",
        "\n",
        "# Verify loaded model works\n",
        "test_sample = X_test.iloc[:5]\n",
        "predictions_sample = predict_batch(test_sample, loaded_model, loaded_preprocessor)\n",
        "print(\"\\nTest prediction with loaded model:\")\n",
        "print(predictions_sample[['step', 'type', 'amount', 'fraud_probability', 'is_fraud']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Flask API (Code Reference)\n",
        "**Source Files**: `api/app.py`, `api/predict_endpoint.py`\n",
        "\n",
        "Example code for Flask API deployment (for reference - not executable in notebook).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FLASK API CODE (Reference Only - Not Executable in Notebook)\n",
        "# Source: api/app.py, api/predict_endpoint.py\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Flask API Application Structure:\n",
        "\n",
        "# api/app.py\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from api.predict_endpoint import predict_single, predict_batch, validate_transaction\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\"}), 200\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json()\n",
        "    result = predict_single(data)\n",
        "    return jsonify(result), 200\n",
        "\n",
        "@app.route('/predict_batch', methods=['POST'])\n",
        "def predict_batch_endpoint():\n",
        "    data = request.get_json()\n",
        "    results = predict_batch(data)\n",
        "    return jsonify({\"count\": len(results), \"results\": results}), 200\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host='0.0.0.0', port=5000)\n",
        "\n",
        "# api/predict_endpoint.py\n",
        "def predict_single(transaction_data):\n",
        "    # Load model artifacts (done once at startup)\n",
        "    # Preprocess transaction\n",
        "    # Make prediction\n",
        "    return {\n",
        "        \"transaction_id\": transaction_data.get('step'),\n",
        "        \"is_fraud\": bool(prediction),\n",
        "        \"fraud_probability\": float(prob),\n",
        "        \"confidence\": \"high\" if prob > 0.8 or prob < 0.2 else \"medium\"\n",
        "    }\n",
        "\n",
        "# Usage Example:\n",
        "# import requests\n",
        "# response = requests.post('http://localhost:5000/predict', json={\n",
        "#     \"step\": 1,\n",
        "#     \"type\": \"TRANSFER\",\n",
        "#     \"amount\": 181.0,\n",
        "#     ...\n",
        "# })\n",
        "\"\"\"\n",
        "\n",
        "print(\"Flask API code structure shown above.\")\n",
        "print(\"\\nTo run the API:\")\n",
        "print(\"1. Save model artifacts (done above)\")\n",
        "print(\"2. Run: python api/app.py\")\n",
        "print(\"3. API will be available at http://localhost:5000\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates the complete end-to-end ML pipeline for fraud detection:\n",
        "\n",
        "1.  **Configuration Setup** - Project paths and hyperparameters\n",
        "2.  **Data Loading** - Extract from SQLite database with chunking\n",
        "3.  **Data Splitting** - Train/eval/test splits preserving temporal order\n",
        "4.  **Feature Engineering** - Balance, transaction, time, and account features\n",
        "5.  **Preprocessing** - Sklearn-compatible pipeline with encoding and scaling\n",
        "6.  **Model Training** - XGBoost with SMOTE for class imbalance\n",
        "7.  **Model Evaluation** - Comprehensive metrics and visualizations\n",
        "8.  **Predictions** - Batch predictions on new data\n",
        "9.  **Model Persistence** - Save and load model artifacts\n",
        "10.  **API Deployment** - Flask API structure (reference)\n",
        "\n",
        "All code is organized to match the project structure, making it easy to understand the flow from data ingestion to production deployment.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
