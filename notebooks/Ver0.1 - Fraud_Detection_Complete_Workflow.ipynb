{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection ML Pipeline - Complete Workflow\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the complete end-to-end machine learning pipeline for fraud detection, consolidating all modules and workflows from the project.\n",
    "\n",
    "### Project Structure:\n",
    "```\n",
    "1.Fraud_Detection/\n",
    "├── config/           # Configuration parameters\n",
    "├── src/\n",
    "│   ├── data/         # Data loading and splitting\n",
    "│   ├── preprocessing/ # Feature engineering and preprocessing\n",
    "│   ├── models/       # Model training\n",
    "│   ├── evaluation/   # Model evaluation\n",
    "│   └── utils/        # Utilities\n",
    "├── scripts/          # Training and prediction scripts\n",
    "├── api/             # Flask API for deployment\n",
    "├── models/          # Saved model artifacts\n",
    "├── reports/         # Evaluation reports and plots\n",
    "└── notebooks/       # Analysis notebooks\n",
    "```\n",
    "\n",
    "### Workflow Sections:\n",
    "1. **Configuration Setup** - Project paths and hyperparameters\n",
    "2. **Data Ingestion** - Load data from SQLite database\n",
    "3. **Exploratory Data Analysis (EDA)** - Understand the data\n",
    "4. **Data Splitting** - Train/eval/test splits\n",
    "5. **Feature Engineering** - Create derived features\n",
    "6. **Data Preprocessing** - Encoding, scaling, imputation\n",
    "7. **Model Training** - XGBoost with class imbalance handling\n",
    "8. **Model Evaluation** - Comprehensive metrics and visualizations\n",
    "9. **Predictions** - Batch and real-time predictions\n",
    "10. **Model Persistence** - Save and load artifacts\n",
    "11. **API Deployment** - Flask API structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Configuration Setup\n",
    "**Source File**: `config/config.py`\n",
    "\n",
    "Setting up project paths, database connections, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# Source: config/config.py\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    precision_recall_curve, roc_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model interpretation\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP not available\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not available\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Project Configuration\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATABASE_PATH = PROJECT_ROOT.parent.parent.parent / \"Database.db\"\n",
    "\n",
    "# Data split configuration\n",
    "TRAIN_SIZE = 4_000_000  # First 4M records for training\n",
    "EVAL_SIZE = 1_000_000   # Next 1M records for evaluation\n",
    "\n",
    "# Directory paths\n",
    "DIRECTORIES = {\n",
    "    \"models\": PROJECT_ROOT / \"models\",\n",
    "    \"logs\": PROJECT_ROOT / \"logs\",\n",
    "    \"reports\": PROJECT_ROOT / \"reports\",\n",
    "    \"notebooks\": PROJECT_ROOT / \"notebooks\",\n",
    "    \"data\": PROJECT_ROOT / \"data\",\n",
    "}\n",
    "\n",
    "# Model file paths\n",
    "MODEL_PATHS = {\n",
    "    \"preprocessor\": DIRECTORIES[\"models\"] / \"preprocessor.pkl\",\n",
    "    \"model\": DIRECTORIES[\"models\"] / \"model.pkl\",\n",
    "    \"feature_names\": DIRECTORIES[\"models\"] / \"feature_names.pkl\",\n",
    "}\n",
    "\n",
    "# Column type mappings\n",
    "COLUMN_TYPES = {\n",
    "    \"step\": \"int64\",\n",
    "    \"type\": \"category\",\n",
    "    \"amount\": \"float64\",\n",
    "    \"nameOrig\": \"string\",\n",
    "    \"oldbalanceOrg\": \"float64\",\n",
    "    \"newbalanceOrig\": \"float64\",\n",
    "    \"nameDest\": \"string\",\n",
    "    \"oldbalanceDest\": \"float64\",\n",
    "    \"newbalanceDest\": \"float64\",\n",
    "    \"isFraud\": \"int64\",\n",
    "    \"isFlaggedFraud\": \"int64\",\n",
    "}\n",
    "\n",
    "# Database table name\n",
    "DB_TABLE_NAME = \"Fraud_detection\"\n",
    "\n",
    "# Model hyperparameters\n",
    "MODEL_CONFIG = {\n",
    "    \"primary_model\": \"xgboost\",\n",
    "    \"use_smote\": True,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.2,\n",
    "    \"cv_folds\": 5,\n",
    "    \"scoring_metric\": \"roc_auc\",\n",
    "}\n",
    "\n",
    "# XGBoost hyperparameters\n",
    "XGBOOST_PARAMS = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 100,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 3,\n",
    "    \"scale_pos_weight\": 100,  # For class imbalance\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Feature engineering configuration\n",
    "FEATURE_CONFIG = {\n",
    "    \"use_account_frequency\": False,\n",
    "    \"use_time_features\": True,\n",
    "    \"use_balance_features\": True,\n",
    "    \"use_transaction_features\": True,\n",
    "}\n",
    "\n",
    "# SMOTE configuration\n",
    "SMOTE_CONFIG = {\n",
    "    \"k_neighbors\": 5,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Evaluation configuration\n",
    "EVALUATION_CONFIG = {\n",
    "    \"optimize_threshold\": True,\n",
    "    \"target_recall\": 0.90,  # Target recall for fraud detection\n",
    "    \"precision_weight\": 0.3,\n",
    "    \"recall_weight\": 0.7,\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in DIRECTORIES.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded!\")\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Database Path: {DATABASE_PATH}\")\n",
    "print(f\"Database exists: {DATABASE_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Ingestion\n",
    "**Source File**: `src/data/data_loader.py`\n",
    "\n",
    "Functions to load data from SQLite database with chunking for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# Source: src/data/data_loader.py\n",
    "# ============================================================================\n",
    "\n",
    "def load_data_from_db(\n",
    "    db_path=None,\n",
    "    table_name=None,\n",
    "    chunk_size=100000,\n",
    "    max_rows=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load data from SQLite database with chunking for large datasets.\n",
    "    \"\"\"\n",
    "    if db_path is None:\n",
    "        db_path = DATABASE_PATH\n",
    "    if table_name is None:\n",
    "        table_name = DB_TABLE_NAME\n",
    "    \n",
    "    print(f\"Loading data from {db_path} table {table_name}\")\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        \n",
    "        # Get total row count\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        total_rows = cursor.fetchone()[0]\n",
    "        print(f\"Total rows in table: {total_rows:,}\")\n",
    "        \n",
    "        # Determine how many rows to load\n",
    "        rows_to_load = min(total_rows, max_rows) if max_rows else total_rows\n",
    "        \n",
    "        # Load data in chunks\n",
    "        chunks = []\n",
    "        offset = 0\n",
    "        \n",
    "        while offset < rows_to_load:\n",
    "            current_chunk_size = min(chunk_size, rows_to_load - offset)\n",
    "            query = f\"SELECT * FROM {table_name} LIMIT {current_chunk_size} OFFSET {offset}\"\n",
    "            \n",
    "            chunk = pd.read_sql_query(query, conn)\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            offset += current_chunk_size\n",
    "            if offset % 500000 == 0:\n",
    "                print(f\"Loaded {offset:,}/{rows_to_load:,} rows\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Concatenate all chunks\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        print(f\"Successfully loaded {len(df):,} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_column_types(df, column_types=None):\n",
    "    \"\"\"Convert column types according to schema.\"\"\"\n",
    "    if column_types is None:\n",
    "        column_types = COLUMN_TYPES\n",
    "    \n",
    "    print(\"Converting column types\")\n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    for column, dtype in column_types.items():\n",
    "        if column in df_converted.columns:\n",
    "            try:\n",
    "                if dtype == \"float64\":\n",
    "                    df_converted[column] = pd.to_numeric(df_converted[column], errors=\"coerce\")\n",
    "                elif dtype == \"int64\":\n",
    "                    df_converted[column] = pd.to_numeric(df_converted[column], errors=\"coerce\").astype(\"Int64\")\n",
    "                elif dtype == \"category\":\n",
    "                    df_converted[column] = df_converted[column].astype(\"category\")\n",
    "                elif dtype == \"string\":\n",
    "                    df_converted[column] = df_converted[column].astype(\"string\")\n",
    "                else:\n",
    "                    df_converted[column] = df_converted[column].astype(dtype)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to convert {column} to {dtype}: {str(e)}\")\n",
    "    \n",
    "    return df_converted\n",
    "\n",
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"Perform basic data validation checks.\"\"\"\n",
    "    print(\"Validating data\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_columns = [\"step\", \"type\", \"amount\", \"nameOrig\", \"oldbalanceOrg\", \n",
    "                        \"newbalanceOrig\", \"nameDest\", \"oldbalanceDest\", \n",
    "                        \"newbalanceDest\", \"isFraud\", \"isFlaggedFraud\"]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Check for empty dataframe\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "    \n",
    "    # Log basic statistics\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Fraud class distribution:\\n{df['isFraud'].value_counts()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_prepare_data(\n",
    "    db_path=None,\n",
    "    table_name=None,\n",
    "    chunk_size=100000,\n",
    "    max_rows=None,\n",
    "    convert_types=True,\n",
    "    validate=True\n",
    "):\n",
    "    \"\"\"Complete data loading pipeline: load, convert types, and validate.\"\"\"\n",
    "    print(\"Starting data loading pipeline\")\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data_from_db(db_path, table_name, chunk_size, max_rows)\n",
    "    \n",
    "    # Convert types\n",
    "    if convert_types:\n",
    "        df = convert_column_types(df)\n",
    "    \n",
    "    # Validate\n",
    "    if validate:\n",
    "        df = validate_data(df)\n",
    "    \n",
    "    print(\"Data loading pipeline completed successfully\")\n",
    "    return df\n",
    "\n",
    "# Load sample data for demonstration (use max_rows to limit for faster execution)\n",
    "print(\"Loading data sample...\")\n",
    "df = load_and_prepare_data(max_rows=50000)  # Limit for notebook demo\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Understanding the data distribution, patterns, and fraud characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"Perform comprehensive exploratory data analysis.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic information\n",
    "    print(\"\\n1. Basic Data Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n2. Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0] if missing.any() else \"No missing values\")\n",
    "    \n",
    "    # Class distribution\n",
    "    print(\"\\n3. Fraud Class Distribution:\")\n",
    "    fraud_counts = df['isFraud'].value_counts()\n",
    "    fraud_pct = df['isFraud'].value_counts(normalize=True) * 100\n",
    "    print(f\"Counts:\\n{fraud_counts}\")\n",
    "    print(f\"Percentages:\\n{fraud_pct.round(2)}\")\n",
    "    print(f\"Imbalance ratio: {fraud_counts.min() / fraud_counts.max():.6f}\")\n",
    "    \n",
    "    # Transaction types\n",
    "    print(\"\\n4. Transaction Types:\")\n",
    "    type_counts = df['type'].value_counts()\n",
    "    print(type_counts)\n",
    "    \n",
    "    # Fraud by transaction type\n",
    "    print(\"\\n5. Fraud by Transaction Type:\")\n",
    "    fraud_by_type = df.groupby('type')['isFraud'].agg(['count', 'sum', 'mean'])\n",
    "    fraud_by_type.columns = ['Total', 'Fraud', 'Fraud_Rate']\n",
    "    fraud_by_type['Fraud_Rate'] = (fraud_by_type['Fraud_Rate'] * 100).round(2)\n",
    "    print(fraud_by_type)\n",
    "    \n",
    "    # Amount statistics\n",
    "    print(\"\\n6. Amount Statistics:\")\n",
    "    print(df['amount'].describe())\n",
    "    \n",
    "    # Amount by fraud status\n",
    "    print(\"\\n7. Amount by Fraud Status:\")\n",
    "    amount_stats = df.groupby('isFraud')['amount'].agg(['mean', 'median', 'std', 'max'])\n",
    "    print(amount_stats)\n",
    "    \n",
    "    return fraud_counts, fraud_by_type, amount_stats\n",
    "\n",
    "\n",
    "def create_eda_visualizations(df):\n",
    "    \"\"\"Create EDA visualizations.\"\"\"\n",
    "    reports_dir = DIRECTORIES[\"reports\"]\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Fraud Detection - Exploratory Data Analysis', fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. Class distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.countplot(data=df, x='isFraud', ax=ax1)\n",
    "    ax1.set_title('Fraud Class Distribution')\n",
    "    ax1.set_xlabel('Is Fraud')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(df)\n",
    "    for p in ax1.patches:\n",
    "        percentage = f'{100 * p.get_height() / total:.2f}%'\n",
    "        ax1.annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Transaction types\n",
    "    ax2 = axes[0, 1]\n",
    "    type_counts = df['type'].value_counts()\n",
    "    ax2.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "    ax2.set_title('Transaction Types Distribution')\n",
    "    \n",
    "    # 3. Amount distribution (log scale)\n",
    "    ax3 = axes[0, 2]\n",
    "    sns.histplot(data=df, x='amount', log_scale=True, bins=50, ax=ax3)\n",
    "    ax3.set_title('Transaction Amount Distribution (Log Scale)')\n",
    "    ax3.set_xlabel('Amount (log scale)')\n",
    "    ax3.set_ylabel('Count')\n",
    "    \n",
    "    # 4. Amount by fraud status\n",
    "    ax4 = axes[1, 0]\n",
    "    sns.boxplot(data=df, x='isFraud', y='amount', ax=ax4)\n",
    "    ax4.set_title('Transaction Amount by Fraud Status')\n",
    "    ax4.set_xlabel('Is Fraud')\n",
    "    ax4.set_ylabel('Amount')\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    # 5. Fraud rate by transaction type\n",
    "    ax5 = axes[1, 1]\n",
    "    fraud_rate_by_type = df.groupby('type')['isFraud'].mean() * 100\n",
    "    fraud_rate_by_type.sort_values(ascending=False).plot(kind='bar', ax=ax5)\n",
    "    ax5.set_title('Fraud Rate by Transaction Type')\n",
    "    ax5.set_xlabel('Transaction Type')\n",
    "    ax5.set_ylabel('Fraud Rate (%)')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 6. Time series (step) analysis\n",
    "    ax6 = axes[1, 2]\n",
    "    fraud_by_step = df.groupby('step')['isFraud'].mean().rolling(window=24).mean()\n",
    "    ax6.plot(fraud_by_step.index, fraud_by_step.values)\n",
    "    ax6.set_title('Fraud Rate Over Time (24-hour rolling average)')\n",
    "    ax6.set_xlabel('Step (Hour)')\n",
    "    ax6.set_ylabel('Fraud Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports_dir / \"eda_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional detailed analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Balance analysis\n",
    "    ax1 = axes[0]\n",
    "    df_sample = df.sample(min(10000, len(df)), random_state=42)\n",
    "    sns.scatterplot(data=df_sample, x='oldbalanceOrg', y='newbalanceOrig', \n",
    "                    hue='isFraud', alpha=0.6, ax=ax1)\n",
    "    ax1.set_title('Origin Account Balance Changes')\n",
    "    ax1.set_xlabel('Old Balance Origin')\n",
    "    ax1.set_ylabel('New Balance Origin')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Hour of day analysis\n",
    "    ax2 = axes[1]\n",
    "    if 'step' in df.columns:\n",
    "        df['hour_of_day'] = df['step'] % 24\n",
    "        fraud_by_hour = df.groupby('hour_of_day')['isFraud'].mean() * 100\n",
    "        fraud_by_hour.plot(kind='bar', ax=ax2)\n",
    "        ax2.set_title('Fraud Rate by Hour of Day')\n",
    "        ax2.set_xlabel('Hour of Day')\n",
    "        ax2.set_ylabel('Fraud Rate (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports_dir / \"eda_detailed.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Perform EDA\n",
    "fraud_counts, fraud_by_type, amount_stats = perform_eda(df)\n",
    "\n",
    "# Create visualizations\n",
    "create_eda_visualizations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Splitting\n",
    "**Source File**: `src/data/data_splitter.py`\n",
    "\n",
    "Split dataset into train, evaluation, and test sets with proper stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA SPLITTING FUNCTIONS\n",
    "# Source: src/data/data_splitter.py\n",
    "# ============================================================================\n",
    "\n",
    "def split_data(\n",
    "    df,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    eval_size=EVAL_SIZE,\n",
    "    random_state=42,\n",
    "    preserve_order=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Split dataset into train, evaluation, and test sets.\n",
    "    \"\"\"\n",
    "    print(f\"Splitting data: train={train_size:,}, eval={eval_size:,}\")\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    \n",
    "    if preserve_order and 'step' in df.columns:\n",
    "        # Sort by step to preserve temporal order\n",
    "        df_sorted = df.sort_values('step').reset_index(drop=True)\n",
    "        print(\"Preserving temporal order based on 'step' column\")\n",
    "    else:\n",
    "        df_sorted = df.copy()\n",
    "        if not preserve_order:\n",
    "            # Shuffle for random split\n",
    "            df_sorted = df_sorted.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "            print(\"Randomly shuffling data\")\n",
    "    \n",
    "    # Calculate split indices\n",
    "    train_end = min(train_size, total_rows)\n",
    "    eval_end = min(train_size + eval_size, total_rows)\n",
    "    \n",
    "    # Split data\n",
    "    train_df = df_sorted.iloc[:train_end].copy()\n",
    "    eval_df = df_sorted.iloc[train_end:eval_end].copy()\n",
    "    test_df = df_sorted.iloc[eval_end:].copy()\n",
    "    \n",
    "    print(f\"Train set: {len(train_df):,} rows\")\n",
    "    print(f\"Eval set: {len(eval_df):,} rows\")\n",
    "    print(f\"Test set: {len(test_df):,} rows\")\n",
    "    \n",
    "    # Log class distribution for each split\n",
    "    if 'isFraud' in train_df.columns:\n",
    "        print(f\"\\nTrain fraud distribution:\\n{train_df['isFraud'].value_counts()}\")\n",
    "        print(f\"Eval fraud distribution:\\n{eval_df['isFraud'].value_counts()}\")\n",
    "        print(f\"Test fraud distribution:\\n{test_df['isFraud'].value_counts()}\")\n",
    "    \n",
    "    return train_df, eval_df, test_df\n",
    "\n",
    "\n",
    "def split_features_target(df, target_column=\"isFraud\"):\n",
    "    \"\"\"\n",
    "    Split DataFrame into features and target.\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame\")\n",
    "    \n",
    "    # Drop target and other non-feature columns\n",
    "    columns_to_drop = [\n",
    "        target_column,\n",
    "        'isFlaggedFraud',  # Business flag, not a feature\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    \n",
    "    X = df.drop(columns=columns_to_drop).copy()\n",
    "    y = df[target_column].copy()\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "    print(f\"Feature columns: {list(X.columns)}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_stratified_split_info(df, target_column=\"isFraud\"):\n",
    "    \"\"\"\n",
    "    Get information about class distribution for stratified splitting.\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found\")\n",
    "    \n",
    "    class_counts = df[target_column].value_counts().sort_index()\n",
    "    class_proportions = df[target_column].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    info = {\n",
    "        \"class_counts\": class_counts.to_dict(),\n",
    "        \"class_proportions\": class_proportions.to_dict(),\n",
    "        \"total_samples\": len(df),\n",
    "        \"n_classes\": len(class_counts),\n",
    "        \"imbalance_ratio\": class_counts.min() / class_counts.max() if len(class_counts) > 1 else 1.0\n",
    "    }\n",
    "    \n",
    "    print(f\"Class distribution: {info['class_counts']}\")\n",
    "    print(f\"Imbalance ratio: {info['imbalance_ratio']:.6f}\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "# Split the loaded data\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get class distribution info\n",
    "split_info = get_stratified_split_info(df)\n",
    "\n",
    "# Split data (adjust sizes for demo)\n",
    "train_df, eval_df, test_df = split_data(\n",
    "    df,\n",
    "    train_size=int(len(df) * 0.6),  # Adjusted for demo\n",
    "    eval_size=int(len(df) * 0.2),\n",
    "    preserve_order=True\n",
    ")\n",
    "\n",
    "# Split features and target\n",
    "X_train, y_train = split_features_target(train_df)\n",
    "X_eval, y_eval = split_features_target(eval_df)\n",
    "X_test, y_test = split_features_target(test_df)\n",
    "\n",
    "print(f\"\\nFinal splits:\")\n",
    "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Evaluation set: X={X_eval.shape}, y={y_eval.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Engineering\n",
    "**Source File**: `src/preprocessing/feature_engineering.py`\n",
    "\n",
    "Create derived features from raw transaction data including balance, transaction, time, and account features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING FUNCTIONS\n",
    "# Source: src/preprocessing/feature_engineering.py\n",
    "# ============================================================================\n",
    "\n",
    "def create_balance_features(df):\n",
    "    \"\"\"Create balance-related features.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Balance differences\n",
    "    df_features['balance_diff_orig'] = (\n",
    "        df_features['oldbalanceOrg'] - df_features['newbalanceOrig']\n",
    "    )\n",
    "    df_features['balance_diff_dest'] = (\n",
    "        df_features['newbalanceDest'] - df_features['oldbalanceDest']\n",
    "    )\n",
    "    \n",
    "    # Zero balance flags\n",
    "    df_features['balance_orig_zero'] = (df_features['oldbalanceOrg'] == 0).astype(int)\n",
    "    df_features['balance_dest_zero'] = (df_features['oldbalanceDest'] == 0).astype(int)\n",
    "    \n",
    "    # Zero balance after transaction\n",
    "    df_features['zero_balance_after_transaction'] = (\n",
    "        df_features['newbalanceOrig'] == 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Balance ratios\n",
    "    df_features['balance_orig_ratio'] = np.where(\n",
    "        df_features['oldbalanceOrg'] > 0,\n",
    "        df_features['amount'] / df_features['oldbalanceOrg'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['balance_dest_ratio'] = np.where(\n",
    "        df_features['oldbalanceDest'] > 0,\n",
    "        df_features['amount'] / (df_features['oldbalanceDest'] + 1),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def create_transaction_features(df):\n",
    "    \"\"\"Create transaction-related features.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Log amount (handle zero and negative)\n",
    "    df_features['amount_log'] = np.log1p(df_features['amount'].clip(lower=0))\n",
    "    \n",
    "    # Amount per original balance\n",
    "    df_features['amount_per_balance_orig'] = (\n",
    "        df_features['amount'] / (df_features['oldbalanceOrg'] + 1)\n",
    "    )\n",
    "    \n",
    "    # Check if transaction empties origin account\n",
    "    df_features['empties_origin'] = (\n",
    "        (df_features['oldbalanceOrg'] > 0) & \n",
    "        (df_features['newbalanceOrig'] == 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Check if transaction creates new destination balance\n",
    "    df_features['creates_dest_balance'] = (\n",
    "        (df_features['oldbalanceDest'] == 0) & \n",
    "        (df_features['newbalanceDest'] > 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Amount categories (buckets)\n",
    "    df_features['amount_category'] = pd.cut(\n",
    "        df_features['amount'],\n",
    "        bins=[0, 100, 1000, 10000, 100000, float('inf')],\n",
    "        labels=['very_small', 'small', 'medium', 'large', 'very_large'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create time-related features from step column.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    if 'step' not in df_features.columns:\n",
    "        print(\"Warning: 'step' column not found, skipping time features\")\n",
    "        return df_features\n",
    "    \n",
    "    # Hour of day (step represents hours)\n",
    "    df_features['hour_of_day'] = df_features['step'] % 24\n",
    "    \n",
    "    # Day of week (assuming step 0 is start of week)\n",
    "    df_features['day_of_week'] = (df_features['step'] // 24) % 7\n",
    "    \n",
    "    # Is weekend\n",
    "    df_features['is_weekend'] = (\n",
    "        (df_features['day_of_week'] == 5) | (df_features['day_of_week'] == 6)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Is business hours (9-17)\n",
    "    df_features['is_business_hours'] = (\n",
    "        (df_features['hour_of_day'] >= 9) & (df_features['hour_of_day'] < 17)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Is night (22-6)\n",
    "    df_features['is_night'] = (\n",
    "        (df_features['hour_of_day'] >= 22) | (df_features['hour_of_day'] < 6)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def create_account_features(df, use_frequency=False):\n",
    "    \"\"\"Create account-related features.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Same account transfer flag\n",
    "    df_features['same_account_transfer'] = (\n",
    "        df_features['nameOrig'] == df_features['nameDest']\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Account name prefixes (C = customer, M = merchant)\n",
    "    df_features['orig_is_customer'] = (\n",
    "        df_features['nameOrig'].str.startswith('C', na=False)\n",
    "    ).astype(int)\n",
    "    df_features['dest_is_customer'] = (\n",
    "        df_features['nameDest'].str.startswith('C', na=False)\n",
    "    ).astype(int)\n",
    "    \n",
    "    if use_frequency:\n",
    "        print(\"Computing account frequency features (this may take time)\")\n",
    "        # Frequency of origin account\n",
    "        orig_counts = df_features['nameOrig'].value_counts()\n",
    "        df_features['orig_account_frequency'] = df_features['nameOrig'].map(orig_counts)\n",
    "        \n",
    "        # Frequency of destination account\n",
    "        dest_counts = df_features['nameDest'].value_counts()\n",
    "        df_features['dest_account_frequency'] = df_features['nameDest'].map(dest_counts)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def create_all_features(df, feature_config=None):\n",
    "    \"\"\"Create all engineered features.\"\"\"\n",
    "    if feature_config is None:\n",
    "        feature_config = FEATURE_CONFIG\n",
    "    \n",
    "    print(\"Starting feature engineering pipeline\")\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Balance features\n",
    "    if feature_config.get(\"use_balance_features\", True):\n",
    "        df_features = create_balance_features(df_features)\n",
    "    \n",
    "    # Transaction features\n",
    "    if feature_config.get(\"use_transaction_features\", True):\n",
    "        df_features = create_transaction_features(df_features)\n",
    "    \n",
    "    # Time features\n",
    "    if feature_config.get(\"use_time_features\", True):\n",
    "        df_features = create_time_features(df_features)\n",
    "    \n",
    "    # Account features\n",
    "    df_features = create_account_features(\n",
    "        df_features,\n",
    "        use_frequency=feature_config.get(\"use_account_frequency\", False)\n",
    "    )\n",
    "    \n",
    "    print(f\"Feature engineering complete. Shape: {df_features.shape}\")\n",
    "    new_features = [col for col in df_features.columns if col not in df.columns]\n",
    "    print(f\"New feature columns: {len(new_features)} features created\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def analyze_feature_importance_raw(X_original, X_featured, y):\n",
    "    \"\"\"Analyze the importance of engineered features.\"\"\"\n",
    "    print(\"\\nFeature Engineering Analysis:\")\n",
    "    print(f\"Original features: {len(X_original.columns)}\")\n",
    "    print(f\"After engineering: {len(X_featured.columns)}\")\n",
    "    \n",
    "    new_features = [col for col in X_featured.columns if col not in X_original.columns]\n",
    "    print(f\"New engineered features: {len(new_features)}\")\n",
    "    print(f\"Engineered features: {new_features}\")\n",
    "    \n",
    "    # Quick correlation analysis\n",
    "    if len(new_features) > 0:\n",
    "        engineered_corr = X_featured[new_features].corrwith(y).abs().sort_values(ascending=False)\n",
    "        print(f\"\\nTop engineered features by correlation with target:\")\n",
    "        print(engineered_corr.head(10))\n",
    "\n",
    "\n",
    "# Apply feature engineering to training data\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "X_train_features = create_all_features(X_train)\n",
    "\n",
    "# Analyze feature importance\n",
    "analyze_feature_importance_raw(X_train, X_train_features, y_train)\n",
    "\n",
    "print(f\"\\nOriginal columns: {len(X_train.columns)}\")\n",
    "print(f\"Features after engineering: {len(X_train_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Data Preprocessing\n",
    "**Source File**: `src/preprocessing/preprocessor.py`\n",
    "\n",
    "Sklearn-compatible preprocessing pipeline that handles feature engineering, encoding, scaling, and imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING PIPELINE CLASS\n",
    "# Source: src/preprocessing/preprocessor.py\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FraudDetectionPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Sklearn-compatible preprocessor for fraud detection.\n",
    "    Handles feature engineering, encoding, scaling, and imputation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_config=None,\n",
    "        categorical_columns=None,\n",
    "        numerical_columns=None,\n",
    "        use_one_hot=True,\n",
    "        use_scaling=True\n",
    "    ):\n",
    "        \"\"\"Initialize preprocessor.\"\"\"\n",
    "        self.feature_config = feature_config or FEATURE_CONFIG\n",
    "        self.categorical_columns = categorical_columns or []\n",
    "        self.numerical_columns = numerical_columns or []\n",
    "        self.use_one_hot = use_one_hot\n",
    "        self.use_scaling = use_scaling\n",
    "        \n",
    "        # Transformers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        \n",
    "        # Feature names after transformation\n",
    "        self.feature_names_ = None\n",
    "        self.is_fitted_ = False\n",
    "        \n",
    "    def _identify_columns(self, X):\n",
    "        \"\"\"Identify categorical and numerical columns if not provided.\"\"\"\n",
    "        if not self.categorical_columns and not self.numerical_columns:\n",
    "            for col in X.columns:\n",
    "                if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "                    if col not in self.categorical_columns:\n",
    "                        self.categorical_columns.append(col)\n",
    "                elif X[col].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                    if col not in self.numerical_columns:\n",
    "                        self.numerical_columns.append(col)\n",
    "    \n",
    "    def _get_feature_names(self, X_features):\n",
    "        \"\"\"Get feature names after transformation.\"\"\"\n",
    "        feature_names = []\n",
    "        \n",
    "        # Numerical features\n",
    "        for col in self.numerical_columns:\n",
    "            if col in X_features.columns:\n",
    "                feature_names.append(col)\n",
    "        \n",
    "        # Categorical features\n",
    "        if self.use_one_hot and self.categorical_columns:\n",
    "            # Generate one-hot feature names\n",
    "            for col in self.categorical_columns:\n",
    "                if col in X_features.columns:\n",
    "                    unique_values = X_features[col].unique()\n",
    "                    for value in unique_values[1:]:  # Skip first (dropped)\n",
    "                        feature_names.append(f\"{col}_{value}\")\n",
    "        else:\n",
    "            # Keep original categorical names\n",
    "            for col in self.categorical_columns:\n",
    "                if col in X_features.columns:\n",
    "                    feature_names.append(col)\n",
    "        \n",
    "        return feature_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessor on training data.\"\"\"\n",
    "        print(\"Fitting preprocessor\")\n",
    "        \n",
    "        # Create features\n",
    "        X_features = create_all_features(X, self.feature_config)\n",
    "        \n",
    "        # Identify columns if not already done\n",
    "        self._identify_columns(X_features)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_imputed = X_features.copy()\n",
    "        if self.numerical_columns:\n",
    "            numerical_data = X_imputed[self.numerical_columns].select_dtypes(include=[np.number])\n",
    "            self.imputer.fit(numerical_data)\n",
    "        \n",
    "        # Fit encoders\n",
    "        if self.categorical_columns:\n",
    "            categorical_data = X_imputed[self.categorical_columns]\n",
    "            \n",
    "            if self.use_one_hot:\n",
    "                self.one_hot_encoder.fit(categorical_data)\n",
    "            else:\n",
    "                if len(self.categorical_columns) == 1:\n",
    "                    self.label_encoder.fit(categorical_data[self.categorical_columns[0]])\n",
    "        \n",
    "        # Fit scaler\n",
    "        if self.use_scaling and self.numerical_columns:\n",
    "            numerical_data = X_imputed[self.numerical_columns].select_dtypes(include=[np.number])\n",
    "            if len(numerical_data.columns) > 0:\n",
    "                self.scaler.fit(self.imputer.transform(numerical_data))\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names_ = self._get_feature_names(X_features)\n",
    "        self.is_fitted_ = True\n",
    "        \n",
    "        print(f\"Preprocessor fitted. Output features: {len(self.feature_names_)}\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data using fitted preprocessor.\"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "        \n",
    "        # Create features\n",
    "        X_features = create_all_features(X, self.feature_config)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_processed = X_features.copy()\n",
    "        \n",
    "        # Impute numerical columns\n",
    "        if self.numerical_columns:\n",
    "            numerical_data = X_processed[self.numerical_columns].select_dtypes(include=[np.number])\n",
    "            if len(numerical_data.columns) > 0:\n",
    "                numerical_imputed = self.imputer.transform(numerical_data)\n",
    "                \n",
    "                # Scale if enabled\n",
    "                if self.use_scaling:\n",
    "                    numerical_scaled = self.scaler.transform(numerical_imputed)\n",
    "                else:\n",
    "                    numerical_scaled = numerical_imputed\n",
    "                \n",
    "                # Update DataFrame\n",
    "                for i, col in enumerate(numerical_data.columns):\n",
    "                    X_processed[col] = numerical_scaled[:, i]\n",
    "        \n",
    "        # Encode categorical columns\n",
    "        if self.categorical_columns:\n",
    "            categorical_data = X_processed[self.categorical_columns]\n",
    "            \n",
    "            if self.use_one_hot:\n",
    "                categorical_encoded = self.one_hot_encoder.transform(categorical_data)\n",
    "                categorical_encoded_df = pd.DataFrame(\n",
    "                    categorical_encoded,\n",
    "                    columns=self.one_hot_encoder.get_feature_names_out(self.categorical_columns),\n",
    "                    index=X_processed.index\n",
    "                )\n",
    "                X_processed = X_processed.drop(columns=self.categorical_columns)\n",
    "                X_processed = pd.concat([X_processed, categorical_encoded_df], axis=1)\n",
    "            else:\n",
    "                if len(self.categorical_columns) == 1:\n",
    "                    X_processed[self.categorical_columns[0]] = self.label_encoder.transform(\n",
    "                        categorical_data[self.categorical_columns[0]]\n",
    "                    )\n",
    "        \n",
    "        # Select features in correct order\n",
    "        if self.feature_names_ is not None:\n",
    "            available_features = [f for f in self.feature_names_ if f in X_processed.columns]\n",
    "            X_processed = X_processed[available_features]\n",
    "        \n",
    "        return X_processed\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save preprocessor to file.\"\"\"\n",
    "        print(f\"Saving preprocessor to {filepath}\")\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(self, filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load preprocessor from file.\"\"\"\n",
    "        print(f\"Loading preprocessor from {filepath}\")\n",
    "        return joblib.load(filepath)\n",
    "\n",
    "\n",
    "# Fit and transform the training data\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "preprocessor = FraudDetectionPreprocessor()\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_eval_transformed = preprocessor.transform(X_eval)\n",
    "\n",
    "print(f\"\\nOriginal shape: {X_train.shape}\")\n",
    "print(f\"Transformed shape: {X_train_transformed.shape}\")\n",
    "print(f\"Number of features: {len(X_train_transformed.columns)}\")\n",
    "print(f\"Feature names: {list(X_train_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Training\n",
    "**Source File**: `src/models/model_trainer.py`\n",
    "\n",
    "Train XGBoost model with class imbalance handling using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL TRAINING CLASS\n",
    "# Source: src/models/model_trainer.py\n",
    "# ============================================================================\n",
    "\n",
    "class FraudDetectionModelTrainer:\n",
    "    \"\"\"Model trainer for fraud detection with class imbalance handling.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type=\"xgboost\",\n",
    "        use_smote=True,\n",
    "        smote_config=None,\n",
    "        random_state=42\n",
    "    ):\n",
    "        \"\"\"Initialize model trainer.\"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.use_smote = use_smote\n",
    "        self.smote_config = smote_config or SMOTE_CONFIG\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.model = None\n",
    "        self.smote = None\n",
    "        self.best_params_ = None\n",
    "        \n",
    "        print(f\"Initialized trainer with model_type={model_type}, use_smote={use_smote}\")\n",
    "    \n",
    "    def _create_model(self, params=None):\n",
    "        \"\"\"Create model instance based on model_type.\"\"\"\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        \n",
    "        if self.model_type == \"xgboost\":\n",
    "            default_params = XGBOOST_PARAMS.copy()\n",
    "            default_params.update(params)\n",
    "            default_params['random_state'] = self.random_state\n",
    "            return XGBClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == \"lightgbm\":\n",
    "            default_params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.1,\n",
    "                'n_estimators': 100,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'min_child_samples': 20,\n",
    "                'scale_pos_weight': 100,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "            default_params.update(params)\n",
    "            return LGBMClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == \"random_forest\":\n",
    "            rf_params = {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 20,\n",
    "                'min_samples_split': 5,\n",
    "                'min_samples_leaf': 2,\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': self.random_state,\n",
    "                **params\n",
    "            }\n",
    "            return RandomForestClassifier(**rf_params)\n",
    "        \n",
    "        elif self.model_type == \"logistic\":\n",
    "            lr_params = {\n",
    "                'max_iter': 1000,\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': self.random_state,\n",
    "                **params\n",
    "            }\n",
    "            return LogisticRegression(**lr_params)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {self.model_type}\")\n",
    "    \n",
    "    def _create_smote(self):\n",
    "        \"\"\"Create SMOTE instance.\"\"\"\n",
    "        k_neighbors = self.smote_config.get('k_neighbors', 5)\n",
    "        random_state = self.smote_config.get('random_state', self.random_state)\n",
    "        return SMOTE(\n",
    "            k_neighbors=k_neighbors,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y, tune_hyperparameters=False, n_trials=20):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        print(f\"Training {self.model_type} model\")\n",
    "        print(f\"Training data shape: {X.shape}, Target distribution: {y.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Handle class imbalance with SMOTE\n",
    "        if self.use_smote:\n",
    "            print(\"Applying SMOTE for class imbalance\")\n",
    "            self.smote = self._create_smote()\n",
    "            \n",
    "            min_class_count = y.value_counts().min()\n",
    "            k_neighbors = self.smote_config.get('k_neighbors', 5)\n",
    "            \n",
    "            if min_class_count <= k_neighbors:\n",
    "                print(f\"Not enough samples for SMOTE. Using class_weight instead.\")\n",
    "                self.use_smote = False\n",
    "            else:\n",
    "                try:\n",
    "                    X_resampled, y_resampled = self.smote.fit_resample(X, y)\n",
    "                    print(f\"After SMOTE: {X_resampled.shape}, Target distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
    "                    X, y = X_resampled, y_resampled\n",
    "                except Exception as e:\n",
    "                    print(f\"SMOTE failed: {str(e)}. Using class_weight instead.\")\n",
    "                    self.use_smote = False\n",
    "        \n",
    "        # Create and train model\n",
    "        self.model = self._create_model()\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        self.model.fit(X, y)\n",
    "        print(\"Model training completed\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model to file.\"\"\"\n",
    "        print(f\"Saving model to {filepath}\")\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(self.model, filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load model from file.\"\"\"\n",
    "        print(f\"Loading model from {filepath}\")\n",
    "        return joblib.load(filepath)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "trainer = FraudDetectionModelTrainer(\n",
    "    model_type=MODEL_CONFIG.get(\"primary_model\", \"xgboost\"),\n",
    "    use_smote=MODEL_CONFIG.get(\"use_smote\", True),\n",
    "    random_state=MODEL_CONFIG.get(\"random_state\", 42)\n",
    ")\n",
    "\n",
    "trainer.fit(X_train_transformed, y_train, tune_hyperparameters=False)\n",
    "print(\"\\nModel trained successfully!\")\n",
    "\n",
    "# Save model\n",
    "trainer.save(MODEL_PATHS[\"model\"])\n",
    "print(f\"Model saved to {MODEL_PATHS['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Model Evaluation\n",
    "**Source File**: `src/evaluation/model_evaluator.py`\n",
    "\n",
    "Comprehensive evaluation with metrics, visualizations, and report generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL EVALUATION CLASS\n",
    "# Source: src/evaluation/model_evaluator.py\n",
    "# ============================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation with metrics and visualizations.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor=None, optimize_threshold=True, target_recall=0.90):\n",
    "        \"\"\"Initialize evaluator.\"\"\"\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.optimize_threshold = optimize_threshold\n",
    "        self.target_recall = target_recall\n",
    "        self.optimal_threshold_ = None\n",
    "        self.metrics_ = {}\n",
    "    \n",
    "    def evaluate(self, X, y_true, save_plots=True):\n",
    "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "        print(\"Starting model evaluation\")\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        y_pred = self.model.predict(X)\n",
    "        y_proba = self.model.predict_proba(X)\n",
    "        \n",
    "        # If binary classification, get probabilities for positive class\n",
    "        if y_proba.shape[1] == 2:\n",
    "            y_proba_positive = y_proba[:, 1]\n",
    "        else:\n",
    "            y_proba_positive = y_proba[:, -1]\n",
    "        \n",
    "        # Calculate metrics with default threshold (0.5)\n",
    "        metrics = self._calculate_metrics(y_true, y_pred, y_proba_positive)\n",
    "        self.metrics_ = metrics\n",
    "        \n",
    "        # Optimize threshold if requested\n",
    "        if self.optimize_threshold:\n",
    "            optimal_threshold = self._optimize_threshold(y_true, y_proba_positive)\n",
    "            self.optimal_threshold_ = optimal_threshold\n",
    "            \n",
    "            y_pred_optimal = (y_proba_positive >= optimal_threshold).astype(int)\n",
    "            metrics_optimal = self._calculate_metrics(y_true, y_pred_optimal, y_proba_positive)\n",
    "            metrics['optimal_threshold'] = optimal_threshold\n",
    "            metrics['metrics_at_optimal_threshold'] = metrics_optimal\n",
    "        \n",
    "        # Generate visualizations\n",
    "        if save_plots:\n",
    "            self._generate_plots(X, y_true, y_pred, y_proba_positive, metrics)\n",
    "        \n",
    "        print(\"Model evaluation completed\")\n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred, y_proba):\n",
    "        \"\"\"Calculate all evaluation metrics.\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_true, y_proba),\n",
    "            'pr_auc': average_precision_score(y_true, y_proba),\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "    def _optimize_threshold(self, y_true, y_proba):\n",
    "        \"\"\"Optimize classification threshold based on target recall.\"\"\"\n",
    "        precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        target_idx = np.argmax(recall_vals >= self.target_recall)\n",
    "        \n",
    "        if target_idx > 0:\n",
    "            optimal_threshold = thresholds[target_idx - 1]\n",
    "        else:\n",
    "            optimal_threshold = 0.5\n",
    "        \n",
    "        return optimal_threshold\n",
    "    \n",
    "    def _generate_plots(self, X, y_true, y_pred, y_proba, metrics):\n",
    "        \"\"\"Generate evaluation plots.\"\"\"\n",
    "        reports_dir = DIRECTORIES[\"reports\"]\n",
    "        reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Confusion Matrix')\n",
    "        axes[0, 0].set_xlabel('Predicted')\n",
    "        axes[0, 0].set_ylabel('Actual')\n",
    "        \n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Normalized Confusion Matrix')\n",
    "        axes[0, 1].set_xlabel('Predicted')\n",
    "        axes[0, 1].set_ylabel('Actual')\n",
    "        \n",
    "        # 2. ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        axes[1, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
    "        axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        axes[1, 0].set_xlabel('False Positive Rate')\n",
    "        axes[1, 0].set_ylabel('True Positive Rate')\n",
    "        axes[1, 0].set_title('ROC Curve')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # 3. Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "        axes[1, 1].plot(recall, precision, label=f'PR Curve (AUC = {metrics[\"pr_auc\"]:.4f})')\n",
    "        axes[1, 1].set_xlabel('Recall')\n",
    "        axes[1, 1].set_ylabel('Precision')\n",
    "        axes[1, 1].set_title('Precision-Recall Curve')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(reports_dir / \"evaluation_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # 4. Feature Importance\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': self.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False).head(20)\n",
    "            \n",
    "            sns.barplot(data=feature_importance, y='feature', x='importance')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.title('Top 20 Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(reports_dir / \"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    def generate_classification_report(self, X, y_true, threshold=0.5):\n",
    "        \"\"\"Generate detailed classification report.\"\"\"\n",
    "        y_pred = self.model.predict(X)\n",
    "        y_proba = self.model.predict_proba(X)\n",
    "        \n",
    "        if y_proba.shape[1] == 2:\n",
    "            y_proba_positive = y_proba[:, 1]\n",
    "        else:\n",
    "            y_proba_positive = y_proba[:, -1]\n",
    "        \n",
    "        # Use optimal threshold if available\n",
    "        if self.optimal_threshold_ is not None:\n",
    "            threshold = self.optimal_threshold_\n",
    "            y_pred = (y_proba_positive >= threshold).astype(int)\n",
    "        \n",
    "        report = classification_report(y_true, y_pred, target_names=['Not Fraud', 'Fraud'])\n",
    "        print(f\"Classification Report (threshold={threshold:.4f}):\")\n",
    "        print(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "evaluator = ModelEvaluator(\n",
    "    model=trainer.model,\n",
    "    preprocessor=preprocessor,\n",
    "    optimize_threshold=True,\n",
    "    target_recall=0.90\n",
    ")\n",
    "\n",
    "metrics = evaluator.evaluate(X_eval_transformed, y_eval, save_plots=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC: {metrics['pr_auc']:.4f}\")\n",
    "\n",
    "if 'optimal_threshold' in metrics:\n",
    "    print(f\"\\nOptimal Threshold: {metrics['optimal_threshold']:.4f}\")\n",
    "    print(\"\\nMetrics at Optimal Threshold:\")\n",
    "    opt_metrics = metrics['metrics_at_optimal_threshold']\n",
    "    print(f\"  Accuracy: {opt_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {opt_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {opt_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {opt_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "evaluator.generate_classification_report(X_eval_transformed, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Predictions\n",
    "**Source File**: `scripts/predict.py`\n",
    "\n",
    "Make predictions on new data using the trained model and preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREDICTION FUNCTIONS\n",
    "# Source: scripts/predict.py\n",
    "# ============================================================================\n",
    "\n",
    "def predict_batch(X, model, preprocessor, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make batch predictions on new data.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        threshold: Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions and probabilities\n",
    "    \"\"\"\n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    # Preprocess\n",
    "    X_transformed = preprocessor.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_transformed)\n",
    "    probabilities = model.predict_proba(X_transformed)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = X.copy()\n",
    "    results['predicted_fraud'] = predictions\n",
    "    results['fraud_probability'] = probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0]\n",
    "    results['is_fraud'] = (results['fraud_probability'] >= threshold).astype(int)\n",
    "    \n",
    "    print(f\"Predictions complete: {len(results)} transactions, {results['is_fraud'].sum()} flagged as fraud\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def predict_single(transaction_data, model, preprocessor, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make prediction on a single transaction.\n",
    "    \n",
    "    Args:\n",
    "        transaction_data: Dictionary with transaction features\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        threshold: Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    X = pd.DataFrame([transaction_data])\n",
    "    \n",
    "    # Preprocess\n",
    "    X_transformed = preprocessor.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(X_transformed)[0]\n",
    "    probability = model.predict_proba(X_transformed)[0, 1]\n",
    "    \n",
    "    # Determine confidence level\n",
    "    if probability > 0.8:\n",
    "        confidence = \"high_fraud\"\n",
    "    elif probability > 0.6:\n",
    "        confidence = \"medium_fraud\"\n",
    "    elif probability > 0.4:\n",
    "        confidence = \"low_confidence\"\n",
    "    elif probability > 0.2:\n",
    "        confidence = \"medium_legitimate\"\n",
    "    else:\n",
    "        confidence = \"high_legitimate\"\n",
    "    \n",
    "    result = {\n",
    "        \"prediction\": int(prediction),\n",
    "        \"fraud_probability\": float(probability),\n",
    "        \"is_fraud\": bool(probability >= threshold),\n",
    "        \"confidence\": confidence,\n",
    "        \"threshold_used\": float(threshold)\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def analyze_predictions(predictions_df):\n",
    "    \"\"\"Analyze prediction results.\"\"\"\n",
    "    print(\"\\nPrediction Analysis:\")\n",
    "    print(f\"Total transactions: {len(predictions_df)}\")\n",
    "    print(f\"Predicted fraud: {predictions_df['is_fraud'].sum()}\")\n",
    "    print(f\"Fraud rate: {predictions_df['is_fraud'].mean()*100:.2f}%\")\n",
    "    \n",
    "    # Probability distribution\n",
    "    print(\"\\nFraud Probability Distribution:\")\n",
    "    print(predictions_df['fraud_probability'].describe())\n",
    "    \n",
    "    # High-risk transactions\n",
    "    high_risk = predictions_df[predictions_df['fraud_probability'] > 0.8]\n",
    "    print(f\"\\nHigh-risk transactions (prob > 0.8): {len(high_risk)}\")\n",
    "    \n",
    "    if len(high_risk) > 0:\n",
    "        print(\"High-risk transaction characteristics:\")\n",
    "        print(high_risk[['amount', 'type', 'oldbalanceOrg', 'newbalanceOrig']].describe())\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"=\" * 60)\n",
    "print(\"MAKING PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply feature engineering to test set\n",
    "X_test_features = create_all_features(X_test)\n",
    "X_test_transformed = preprocessor.transform(X_test_features)\n",
    "\n",
    "# Predictions\n",
    "test_predictions = predict_batch(X_test, trainer.model, preprocessor, threshold=0.5)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(test_predictions[['step', 'type', 'amount', 'fraud_probability', 'is_fraud']].head(10))\n",
    "\n",
    "# Analyze predictions\n",
    "analyze_predictions(test_predictions)\n",
    "\n",
    "# Test single prediction\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"SINGLE PREDICTION EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sample_transaction = {\n",
    "    'step': 100,\n",
    "    'type': 'TRANSFER',\n",
    "    'amount': 5000.0,\n",
    "    'nameOrig': 'C123456789',\n",
    "    'oldbalanceOrg': 10000.0,\n",
    "    'newbalanceOrig': 5000.0,\n",
    "    'nameDest': 'C987654321',\n",
    "    'oldbalanceDest': 2000.0,\n",
    "    'newbalanceDest': 7000.0\n",
    "}\n",
    "\n",
    "single_result = predict_single(\n",
    "    sample_transaction, \n",
    "    trainer.model, \n",
    "    preprocessor, \n",
    "    threshold=evaluator.optimal_threshold_ or 0.5\n",
    ")\n",
    "\n",
    "print(\"Single transaction prediction:\")\n",
    "for key, value in single_result.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Model Persistence\n",
    "**Source Files**: `scripts/train_model.py`, `scripts/predict.py`\n",
    "\n",
    "Save and load model artifacts for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE MODEL ARTIFACTS\n",
    "# Source: scripts/train_model.py\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING MODEL ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor.save(MODEL_PATHS[\"preprocessor\"])\n",
    "print(f\"✓ Preprocessor saved to {MODEL_PATHS['preprocessor']}\")\n",
    "\n",
    "# Save model\n",
    "trainer.save(MODEL_PATHS[\"model\"])\n",
    "print(f\"✓ Model saved to {MODEL_PATHS['model']}\")\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(preprocessor.feature_names_, MODEL_PATHS[\"feature_names\"])\n",
    "print(f\"✓ Feature names saved to {MODEL_PATHS['feature_names']}\")\n",
    "\n",
    "# Save optimal threshold\n",
    "if evaluator.optimal_threshold_ is not None:\n",
    "    threshold_path = DIRECTORIES[\"models\"] / \"optimal_threshold.pkl\"\n",
    "    joblib.dump(evaluator.optimal_threshold_, threshold_path)\n",
    "    print(f\"✓ Optimal threshold saved to {threshold_path}\")\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics_path = DIRECTORIES[\"models\"] / \"evaluation_metrics.pkl\"\n",
    "joblib.dump(evaluator.metrics_, metrics_path)\n",
    "print(f\"✓ Evaluation metrics saved to {metrics_path}\")\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL ARTIFACTS (Example for production use)\n",
    "# Source: scripts/predict.py\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING MODEL ARTIFACTS (Example)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load preprocessor\n",
    "loaded_preprocessor = FraudDetectionPreprocessor.load(MODEL_PATHS[\"preprocessor\"])\n",
    "print(\"✓ Preprocessor loaded\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = FraudDetectionModelTrainer.load(MODEL_PATHS[\"model\"])\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# Load feature names\n",
    "loaded_feature_names = joblib.load(MODEL_PATHS[\"feature_names\"])\n",
    "print(f\"✓ Feature names loaded ({len(loaded_feature_names)} features)\")\n",
    "\n",
    "# Load optimal threshold\n",
    "if (DIRECTORIES[\"models\"] / \"optimal_threshold.pkl\").exists():\n",
    "    loaded_threshold = joblib.load(DIRECTORIES[\"models\"] / \"optimal_threshold.pkl\")\n",
    "    print(f\"✓ Optimal threshold loaded: {loaded_threshold:.4f}\")\n",
    "else:\n",
    "    loaded_threshold = 0.5\n",
    "    print(\"Using default threshold: 0.5\")\n",
    "\n",
    "print(\"\\nAll artifacts loaded successfully!\")\n",
    "\n",
    "# Verify loaded model works\n",
    "test_sample = X_test.iloc[:5]\n",
    "predictions_sample = predict_batch(test_sample, loaded_model, loaded_preprocessor, threshold=loaded_threshold)\n",
    "print(\"\\nTest prediction with loaded model:\")\n",
    "print(predictions_sample[['step', 'type', 'amount', 'fraud_probability', 'is_fraud']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: API Deployment\n",
    "**Source Files**: `api/app.py`, `api/predict_endpoint.py`\n",
    "\n",
    "Flask API structure for production deployment (code reference only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FLASK API CODE (Reference Only - Not Executable in Notebook)\n",
    "# Source: api/app.py, api/predict_endpoint.py\n",
    "# ============================================================================\n",
    "\n",
    "flask_api_code = '''\n",
    "# api/app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent))\n",
    "\n",
    "from config.config import MODEL_PATHS, DIRECTORIES\n",
    "from src.preprocessing.preprocessor import FraudDetectionPreprocessor\n",
    "from src.models.model_trainer import FraudDetectionModelTrainer\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Load model artifacts at startup\n",
    "try:\n",
    "    preprocessor = FraudDetectionPreprocessor.load(MODEL_PATHS[\"preprocessor\"])\n",
    "    model = FraudDetectionModelTrainer.load(MODEL_PATHS[\"model\"])\n",
    "    \n",
    "    # Load optimal threshold if available\n",
    "    threshold_path = DIRECTORIES[\"models\"] / \"optimal_threshold.pkl\"\n",
    "    if threshold_path.exists():\n",
    "        optimal_threshold = joblib.load(threshold_path)\n",
    "    else:\n",
    "        optimal_threshold = 0.5\n",
    "    \n",
    "    print(\"✓ Model artifacts loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model artifacts: {str(e)}\")\n",
    "    preprocessor = None\n",
    "    model = None\n",
    "    optimal_threshold = 0.5\n",
    "\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    status = \"healthy\" if model is not None else \"unhealthy\"\n",
    "    return jsonify({\n",
    "        \"status\": status,\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"preprocessor_loaded\": preprocessor is not None\n",
    "    }), 200 if status == \"healthy\" else 503\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_single():\n",
    "    \"\"\"Predict single transaction.\"\"\"\n",
    "    if model is None or preprocessor is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 503\n",
    "    \n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Validate required fields\n",
    "        required_fields = ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', \n",
    "                          'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest']\n",
    "        \n",
    "        missing_fields = [field for field in required_fields if field not in data]\n",
    "        if missing_fields:\n",
    "            return jsonify({\"error\": f\"Missing fields: {missing_fields}\"}), 400\n",
    "        \n",
    "        # Make prediction\n",
    "        result = predict_single(data, model, preprocessor, optimal_threshold)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"transaction_id\": data.get('step'),\n",
    "            \"prediction\": result[\"prediction\"],\n",
    "            \"is_fraud\": result[\"is_fraud\"],\n",
    "            \"fraud_probability\": result[\"fraud_probability\"],\n",
    "            \"confidence\": result[\"confidence\"],\n",
    "            \"threshold_used\": result[\"threshold_used\"]\n",
    "        }), 200\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/predict_batch', methods=['POST'])\n",
    "def predict_batch_endpoint():\n",
    "    \"\"\"Predict multiple transactions.\"\"\"\n",
    "    if model is None or preprocessor is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 503\n",
    "    \n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        transactions = data.get('transactions', [])\n",
    "        \n",
    "        if not transactions:\n",
    "            return jsonify({\"error\": \"No transactions provided\"}), 400\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(transactions)\n",
    "        \n",
    "        # Make predictions\n",
    "        results = predict_batch(df, model, preprocessor, optimal_threshold)\n",
    "        \n",
    "        # Convert to list of dicts\n",
    "        predictions = results.to_dict('records')\n",
    "        \n",
    "        return jsonify({\n",
    "            \"count\": len(predictions),\n",
    "            \"predictions\": predictions\n",
    "        }), 200\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/model_info', methods=['GET'])\n",
    "def model_info():\n",
    "    \"\"\"Get model information.\"\"\"\n",
    "    if model is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 503\n",
    "    \n",
    "    return jsonify({\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"threshold\": optimal_threshold,\n",
    "        \"features_count\": len(preprocessor.feature_names_) if preprocessor else 0,\n",
    "        \"target_recall\": 0.90\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "\n",
    "\n",
    "# api/predict_endpoint.py (helper functions)\n",
    "def predict_single(transaction_data, model, preprocessor, threshold=0.5):\n",
    "    \"\"\"Make prediction on a single transaction.\"\"\"\n",
    "    # Convert to DataFrame\n",
    "    X = pd.DataFrame([transaction_data])\n",
    "    \n",
    "    # Preprocess\n",
    "    X_transformed = preprocessor.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(X_transformed)[0]\n",
    "    probability = model.predict_proba(X_transformed)[0, 1]\n",
    "    \n",
    "    # Determine confidence level\n",
    "    if probability > 0.8:\n",
    "        confidence = \"high_fraud\"\n",
    "    elif probability > 0.6:\n",
    "        confidence = \"medium_fraud\"\n",
    "    elif probability > 0.4:\n",
    "        confidence = \"low_confidence\"\n",
    "    elif probability > 0.2:\n",
    "        confidence = \"medium_legitimate\"\n",
    "    else:\n",
    "        confidence = \"high_legitimate\"\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": int(prediction),\n",
    "        \"fraud_probability\": float(probability),\n",
    "        \"is_fraud\": bool(probability >= threshold),\n",
    "        \"confidence\": confidence,\n",
    "        \"threshold_used\": float(threshold)\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_batch(df, model, preprocessor, threshold=0.5):\n",
    "    \"\"\"Make batch predictions.\"\"\"\n",
    "    # Preprocess\n",
    "    X_transformed = preprocessor.transform(df)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_transformed)\n",
    "    probabilities = model.predict_proba(X_transformed)\n",
    "    \n",
    "    # Create results\n",
    "    results = df.copy()\n",
    "    results['predicted_fraud'] = predictions\n",
    "    results['fraud_probability'] = probabilities[:, 1]\n",
    "    results['is_fraud'] = (results['fraud_probability'] >= threshold).astype(int)\n",
    "    \n",
    "    return results\n",
    "'''\n",
    "\n",
    "print(\"Flask API code structure:\")\n",
    "print(\"\\n📁 API Files Created:\")\n",
    "print(\"  - api/app.py: Main Flask application\")\n",
    "print(\"  - api/predict_endpoint.py: Prediction helper functions\")\n",
    "\n",
    "print(\"\\n🚀 API Endpoints:\")\n",
    "print(\"  - GET /health: Health check\")\n",
    "print(\"  - POST /predict: Single transaction prediction\")\n",
    "print(\"  - POST /predict_batch: Batch predictions\")\n",
    "print(\"  - GET /model_info: Model information\")\n",
    "\n",
    "print(\"\\n💡 Usage Example:\")\n",
    "print(\"\"\"\n",
    "# Single prediction\n",
    "import requests\n",
    "\n",
    "response = requests.post('http://localhost:5000/predict', json={\n",
    "    \"step\": 1,\n",
    "    \"type\": \"TRANSFER\",\n",
    "    \"amount\": 181.0,\n",
    "    \"nameOrig\": \"C123456789\",\n",
    "    \"oldbalanceOrg\": 181.0,\n",
    "    \"newbalanceOrig\": 0.0,\n",
    "    \"nameDest\": \"C987654321\",\n",
    "    \"oldbalanceDest\": 0.0,\n",
    "    \"newbalanceDest\": 181.0\n",
    "})\n",
    "\n",
    "result = response.json()\n",
    "print(f\"Fraud probability: {result['fraud_probability']:.4f}\")\n",
    "print(f\"Is fraud: {result['is_fraud']}\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔧 To run the API:\")\n",
    "print(\"1. Save model artifacts (completed above)\")\n",
    "print(\"2. Install Flask: pip install flask flask-cors\")\n",
    "print(\"3. Run: python api/app.py\")\n",
    "print(\"4. API will be available at http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the complete end-to-end ML pipeline for fraud detection:\n",
    "\n",
    "### ✅ **Completed Workflow Sections:**\n",
    "\n",
    "1. **Configuration Setup** - Project paths, hyperparameters, and settings\n",
    "2. **Data Ingestion** - SQLite database loading with chunking for large datasets\n",
    "3. **Exploratory Data Analysis** - Comprehensive data understanding and visualization\n",
    "4. **Data Splitting** - Train/eval/test splits preserving temporal order\n",
    "5. **Feature Engineering** - Balance, transaction, time, and account features\n",
    "6. **Data Preprocessing** - Sklearn-compatible pipeline with encoding and scaling\n",
    "7. **Model Training** - XGBoost with SMOTE for class imbalance handling\n",
    "8. **Model Evaluation** - Comprehensive metrics, visualizations, and threshold optimization\n",
    "9. **Predictions** - Batch and single transaction predictions\n",
    "10. **Model Persistence** - Save and load model artifacts for production\n",
    "11. **API Deployment** - Flask API structure for real-time predictions\n",
    "\n",
    "### 📁 **Project Structure Understanding:**\n",
    "\n",
    "- **`config/`** - Centralized configuration management\n",
    "- **`src/data/`** - Data loading and splitting utilities\n",
    "- **`src/preprocessing/`** - Feature engineering and preprocessing pipeline\n",
    "- **`src/models/`** - Model training with class imbalance handling\n",
    "- **`src/evaluation/`** - Comprehensive model evaluation and metrics\n",
    "- **`scripts/`** - Standalone training and prediction scripts\n",
    "- **`api/`** - Flask API for production deployment\n",
    "- **`models/`** - Saved model artifacts and preprocessor\n",
    "- **`reports/`** - Evaluation reports, plots, and visualizations\n",
    "- **`notebooks/`** - Analysis and workflow notebooks\n",
    "\n",
    "### 🎯 **Key Features Implemented:**\n",
    "\n",
    "- **Class Imbalance Handling**: SMOTE with fallback to class weights\n",
    "- **Feature Engineering**: Balance ratios, time features, transaction patterns\n",
    "- **Threshold Optimization**: Target recall optimization for fraud detection\n",
    "- **Comprehensive Evaluation**: ROC-AUC, PR-AUC, confusion matrices, feature importance\n",
    "- **Production Ready**: Model persistence, API endpoints, batch processing\n",
    "- **Scalable Design**: Chunked data loading, sklearn-compatible transformers\n",
    "\n",
    "### 🚀 **Next Steps for Production:**\n",
    "\n",
    "1. **Hyperparameter Tuning**: Use Optuna for automated optimization\n",
    "2. **Model Monitoring**: Implement drift detection and performance tracking\n",
    "3. **A/B Testing**: Compare multiple models in production\n",
    "4. **Real-time Processing**: Implement streaming data processing\n",
    "5. **Explainability**: Add SHAP values for model interpretation\n",
    "\n",
    "This consolidated workflow provides a complete foundation for fraud detection that can be easily extended and deployed in production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
